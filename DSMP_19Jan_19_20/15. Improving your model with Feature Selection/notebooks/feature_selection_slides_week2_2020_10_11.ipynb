{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRED\n",
    "Dream11 - App\n",
    "how does dream11 makes money\n",
    "data science at dream11\n",
    "data science at airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100+ raw features\n",
    "30-40 derived features (Aggregates)\n",
    "10 KPI - Final Features\n",
    "Model\n",
    "Feature Selection (Mathematical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curse of dimensionality\n",
    "longer is the time and complexity for CONVERGENCE (best parameters)\n",
    "best coefficinets\n",
    "\n",
    "log(p/(1-p)) = logit(p) = b0 + b1x1 + b2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# To print multiple outputs together\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Change column display number during print\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Feature Selection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![feature_selection](../images/feature_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far\n",
    "\n",
    "***\n",
    "\n",
    "- Basics of Python\n",
    "- Descriptive and Inferential Statistics\n",
    "- Linear Regression\n",
    "- L1/L2 Regularization\n",
    "- Basic data cleaning and Preprocessing\n",
    "- Feature extraction and Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Table of contents\n",
    "***\n",
    "\n",
    "- Importance of Feature Selection\n",
    "    - Variance Threshold\n",
    "    - Pearson\n",
    "    - Select K Best\n",
    "    - f_regression\n",
    "    - RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What is to be covered later?**\n",
    "\n",
    "\n",
    "- Filter Methods\n",
    "- Wrapper Methods\n",
    "- Embedded Methods\n",
    "- Difference between Filter and Wrapper methods\n",
    "- Walk-through example\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So what is Feature Selection?\n",
    "***\n",
    "* In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large rows = big data 1000\n",
    "\n",
    "SAMPLING\n",
    "- SRS\n",
    "- Stratify\n",
    "\n",
    "100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Importance of Feature Selection\n",
    "***\n",
    "* This becomes even more important when the number of features are very large.\n",
    "* You need not use every feature at your disposal for creating an algorithm. \n",
    "* You can assist your algorithm by feeding in only those features that are really important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Algo\n",
    "y = BtX\n",
    "X = 1000 vs 100 vs 10\n",
    "LR - Coeff have some meaning, defined relationships between DV and IV\n",
    "\n",
    "Clustering - Market segmentation\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where to use feature selection?\n",
    "***\n",
    "- It enables the machine learning algorithm to train faster.\n",
    "- It reduces the complexity of a model and makes it easier to interpret.\n",
    "- It improves the accuracy of a model if the right subset is chosen.\n",
    "- It reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Type of Feature Selection\n",
    "***\n",
    "Jay helped John identify different types of feature selection:-\n",
    "\n",
    "* Univariate Feature Selection\n",
    "* Multivariate Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br >\n",
    "\n",
    "## Univariate Feature Selection (1/2)\n",
    "***\n",
    "* Univariate feature selection methods examine -\n",
    "    - the predictive power of individual features\n",
    "    - the strength of the relationship of the feature with the response variable\n",
    "* These methods are simple to run and understand\n",
    "* They also prove to be good for gaining a better understanding of data, and can often be the starting point for multivariate feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corr \n",
    "what kind of relationship --> linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br >\n",
    "\n",
    "## Univariate Feature Selection (2/2)\n",
    "***\n",
    "* We can look at the interaction between the top variables instead of all possible combinations\n",
    "* These methods are not necessarily good for optimizing the feature set for better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continuous variables\n",
    "\n",
    "X1 = [1,1,1,1,1,1,1,1,1,1] = 0\n",
    "X2 = [1,2,3,4,5,6,2,3,4,5] = 2\n",
    "Variance = Knowledge = Information = Data = Pattern\n",
    "\n",
    "select features whose var > threshold = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Method 1 : Removing features with low variance (1/2)\n",
    "***\n",
    "* One of the most basic, yet a very powerful feature selection technique\n",
    "* We want to remove all features whose variance doesn't meet some threshold\n",
    "* For example, a feature with 0 variance means it has the same value for every sample. This means that such a feature would not bring any predictive power to the model. \n",
    "* Hence, we should remove all such zero-variance features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Removing features with low variance (2/2)\n",
    "***\n",
    "* John thought, if he had a dataset with boolean features, and he wanted to remove all features that were either one or zero (on or off) in more than 80% of the samples. How would he do that?\n",
    "\n",
    "* Answer : Boolean features are Bernoulli random variables, and the variance of such variables is given by  Var(x)=p(1−p)\n",
    "\n",
    "* *from sklearn.feature_selection import VarianceThreshold*   is a handy method to remove such features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "X\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".8 * (1 - .8)\n",
    "np.array(X)[:, 0].var()\n",
    "np.array(X)[:, 1].var()\n",
    "np.array(X)[:, 2].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "X\n",
    "sel = VarianceThreshold(threshold=0.1)\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Continuos - Variance\n",
    "Categorical - Entropy ~ Variance \n",
    "Entropy ---> 1, select the column\n",
    "0 to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method 2 : Pearson Correlation\n",
    "***\n",
    "* Another method that Jay suggested was Pearson Correlation\n",
    "* The population correlation coefficient  $ρ_{(X,Y)}$  between two random variables  X and  Y with expected values  $μ_X$  and  $μ_Y$  and standard deviations  $σ_X$ and  $σ_Y$.\n",
    "* It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1. Pearson’s correlation is given as:\n",
    "![FS3.JPG](../images/FS3.JPG)\n",
    "* where E is the expected value operator, cov means covariance, and corr is a widely used alternative notation for the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DV\n",
    "10 IV\n",
    "(DV, IV) - Correlation\n",
    "abs(Correlation) > 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "* One of the simplest method for understanding a feature's relation to the response variable is Pearson correlation coefficient, which measures linear correlation between two variables\n",
    "* Scipy's Pearson's method computes both the correlation and p-value for the correlation, roughly showing the probability of an uncorrelated system creating a correlation value of this magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iowa = pd.read_csv('../data/house_prices_multivariate.csv')\n",
    "iowa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iowa.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iowa.iloc[:,:-1]\n",
    "y = iowa.iloc[:,-1]\n",
    "\n",
    "for i in X.columns:\n",
    "    print(pearsonr(X[i],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 - 0.2 - very weak\n",
    "0.2 - 0.4 - weak \n",
    "0.4 - 0.6 - moderate\n",
    "0.6 - 0.8 - strong\n",
    "> 0.8 - very strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "* With smaller amount of noise, the correlation is relatively strong, with a very low p-value\n",
    "* However, for the noisy comparison, the correlation is very small and p-value is very high\n",
    "* The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation: \n",
    "***\n",
    "* Pearson correlation is the ratio of co-variance of two variables to a product of variance (of the variables)\n",
    "* The correlation coefficient has values between -1 to 1\n",
    "    - A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
    "    - A value closer to 1 implies stronger positive correlation\n",
    "    - A value closer to -1 implies stronger negative correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation is not Transitive:**\n",
    "\n",
    "* Let  X,  Y and  Z be random variables.\n",
    "    - ρ(X,Y) > 0.8\n",
    "    - ρ(Y,Z) > 0.7\n",
    "\n",
    "* Can we say  ρ(X,Z)  will be strongly positive?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A:B\n",
    "    B:C\n",
    "        A:C ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation is not Transitive:**\n",
    "\n",
    "* Not really! \n",
    "* ρX,Z will be positive if ρX,Y and ρY,Z are very close to 1\n",
    "* Mathematically speaking:\n",
    "![FS4.JPG](../images/FS4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Sensitivity to Outliers:**\n",
    "Once again:\n",
    "![FS5.JPG](../images/FS5.JPG)\n",
    "* Means themselves are sensitive to outliers\n",
    "* The correlation itself will be sensitive to outliers as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation doesn't capture nonlinear relationships**\n",
    " \n",
    "* One obvious drawback of Pearson correlation as a feature ranking mechanism is that it is only sensitive to a linear relationship.\n",
    "* If the relation is non-linear, Pearson correlation can be close to zero even if there is a 1-1 correspondence between the two variables.\n",
    "* For example, correlation between x and $x_2$ is zero, when x is centered on 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation doesn't capture nonlinear relationships**\n",
    " \n",
    "* In the image in the next slide, there are several sets of (x, y) points, with the Pearson correlation coefficient of x and y for each set.\n",
    "* Note that the correlation reflects the noisiness and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation doesn't capture nonlinear relationships**\n",
    "\n",
    "![FS6.JPG](../images/FS6.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "*** \n",
    "![FS7.JPG](../images/FS7.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anscombe's dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering Methods (Univariate)\n",
    "1. Variance\n",
    "2. Correlation (Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now that John understood Pearson's Correlation, he wanted to get deeper into Feature Extraction in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Sklearn for Feature Selection\n",
    "***\n",
    "* Scikit-learn has a wide variety of functions for eliminating features based information-criteria (score)\n",
    "* For performing univariate feature selection, we need to specify two parameters\n",
    "    - Selection criteria\n",
    "    - Metric to be used for selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sklearn for Feature Selection**\n",
    "##### Selection Criteria\n",
    "* SelectKBest: Removes all but the k highest scoring features\n",
    "* SelectPercentile: Removes all but a user-specified highest scoring percentage of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sklearn for Feature Selection**\n",
    "##### Regression\n",
    "* **f_regression**: The methods based on F-test estimate the degree of linear dependency between two random variables.\n",
    "* **Mutual_info_regression**:Mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-Score (linear relationship)\n",
    "- F test (Ratio of two variances, one with your feature under consideration, without the feature)\n",
    "\n",
    "X: Take all your features\n",
    "y: Target\n",
    "    \n",
    "(X,y) - Fscore\n",
    "Select the best features based on some threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual_info\n",
    "How much information do I get to know, if I knw the presence of the other feature\n",
    "nonparametric - it has no parameters (they have large parameters = points in dataset)\n",
    "100 rows (10 columns) \n",
    "1000 rows\n",
    "\n",
    "1. KNN\n",
    "2. Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 rows - 100 (distance metric)\n",
    "1000 rows - 1000\n",
    "10k rows - 10k\n",
    "\n",
    "\n",
    "Linear Rgression\n",
    "5 coeff\n",
    "100 rows\n",
    "1000 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sklearn for Feature Selection**\n",
    "\n",
    "##### Selection Metric\n",
    "##### Classification\n",
    "\n",
    "* **chi2**: Based on Chi Squared\n",
    "* **F_classif**: Based on f-test\n",
    "* **Mutual_info_classif**: Based on Mutual Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's understand how f-test and mutual information behave differently. Let's start by creating a toy dataset, in which y depends on three components: a linear, a non-linear and a random.\n",
    "\n",
    "$y = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(67)\n",
    "X = np.random.rand(1000, 3)\n",
    "y = X[:, 0] + np.sin(5* np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape\n",
    "X[:5]\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the importance scores of the features and normalize them so that they can be compared with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "f_test, f_test_pval = f_regression(X, y)\n",
    "f_test\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi\n",
    "mi /= np.max(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's plot these values and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "a = plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    a = plt.subplot(1, 3, i + 1);\n",
    "    a = plt.scatter(X[:, i], y);\n",
    "    a = plt.xlabel(\"$x_{}$\".format(i + 1), fontsize=14);\n",
    "    if i == 0:\n",
    "        a = plt.ylabel(\"$y$\", fontsize=14);\n",
    "    a = plt.title(\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]),\n",
    "              fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can clearly see how f-test can only capture the linear relationship, whereas MI can capture no-linear relationships as well. You can vary the coefficient of the `sin` component to examine how the MI behaves when the coefficient is made sufficiently low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Various methodologies and techniques  to subset  feature space and make models perform better and efficiently.\n",
    "***\n",
    "John got a good measure of feature selection and was more confident than ever on it. He started reading more on techniques to make his models more efficient. This is what he found out-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KPi = f(x1, x2, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "##  Filter Methods\n",
    "***\n",
    "![Filter_1.png](../images/Filter_1.png)\n",
    "[Source: Analytics Vidya Blog](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/) \n",
    "\n",
    "* Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. \n",
    "* Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here.\n",
    "* Examples that we saw just now, are some of the filter mathod techniques.\n",
    "* For basic guidance, you can refer to the following table for defining correlation co-efficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Methods - Independent scores\n",
    "Wrapper Methods - Based on the scores, select subset and run in iteration\n",
    "Embedded Methods - Feature is part of algorithm itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance\n",
    "Correlation (entropy)\n",
    "f_regression\n",
    "mutual_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Filter Methods\n",
    "\n",
    "![FS1.png](../images/FS1.png)\n",
    "[Source: An Analysis of Feature Selection Techniques](http://syllabus.cs.manchester.ac.uk/pgt/2017/COMP61011/goodProjects/Shardlow.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DV - Continuous, IV - Continuous: Correlation (MI)\n",
    "DV - Continuous, IV - Categorical: ANOVA (f_classi)\n",
    "loan_approval, salary\n",
    "yes, 100\n",
    "yes, 150\n",
    "yes, 120\n",
    "\n",
    "no, 60\n",
    "no, 70\n",
    "no, 80\n",
    "\n",
    "Grp1 (loan_approval = true) - 130\n",
    "Grp2 (loan_approval = False) - 70\n",
    "IV = loan_approval ---> Non-Redundant\n",
    "\n",
    "Grp1 (Gender = Male) - 100\n",
    "Grp2 (Gender = Female) - 100\n",
    "IV = Gender ---> Redundant\n",
    "\n",
    "ANOVA - Analysis of variance\n",
    "N categories \n",
    "\n",
    "Color of house : {red green blue}\n",
    "Price of house\n",
    "\n",
    "\n",
    "DV - Continuous, IV - Categorical: ANOVA (f_classi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear Discriminant Analysis - Feature reduction technique for supervised methods\n",
    "N features\n",
    "Transformed space, \n",
    "\n",
    "Classification\n",
    "1. LDA (New Subspace == New Axis) ---> assumption from dataset\n",
    "2. Classifciation algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output - Categorical\n",
    "Input - Continuous\n",
    "Logistic reregssion\n",
    "\n",
    "log(p/1-p) = BtX\n",
    "pvalues, coeff\n",
    "\n",
    "XGBoost\n",
    "1. Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,B,C - old features - 100% variation (data)\n",
    "\n",
    "New_Comp1 = xA + yB + zC\n",
    "New_Comp2 = aA + bB + cC\n",
    "New_Comp3 = dA + eB + fC\n",
    "\n",
    "New_Comp1, New_Comp2, New_Comp3 - 100% variation (data)\n",
    "New_Comp1, New_Comp2 - 99%\n",
    "\n",
    "New_Comp1, New_Comp2,  CategoricalYes/No\n",
    "..., ..., ..., \n",
    "log(p/1-p) = BtX\n",
    "\n",
    "LDA ---> separate classes (sup technique)\n",
    "PCA ---> get max variance (unsup technique)\n",
    "\n",
    "100 features ----> 100 % variation\n",
    "100 Principal Components\n",
    "10 PC ----> 90%\n",
    "90 PC ---> 10%\n",
    "\n",
    "1000 features ----> 30-50 features ---> 95% of variation\n",
    "\n",
    "1. Quicker Convergence\n",
    "\n",
    "\n",
    "Variable Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the result of PCA and LDA\n",
    "100 --> new 100\n",
    "each feature is ging to be a linear combination of all 100 \n",
    "variance explained\n",
    "instead of 100, usage of top 10, 20 lDA components is able to explain 90-95% of variation\n",
    "and hence we can choose only the top X components\n",
    "\n",
    "Original = 3 = X1, X2, X3\n",
    "\n",
    "newLDA1 = aX1 + bX2 + cX3\n",
    "newLDA2 = xX1 + yX2 + zX3\n",
    "newLDA3\n",
    "\n",
    "salesprice = neighbourhood + garagearea + bathroom + living\n",
    "newLDA1 = 0.1*garagearea + 0.9*bathroom + -1.2*living area\n",
    "newLDA2\n",
    "new..LDA4\n",
    "\n",
    "a,b,c,x,y,z = Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neighbourhood, garagearea, salesprice\n",
    "\n",
    "LDA = target categorical\n",
    "PCA = don't have target, target is continuous\n",
    "\n",
    "Objective: Transform from one co-ordinate to another co-ordinate\n",
    "\n",
    "newLDA1, newLDA2, salesprice\n",
    "newPC1, newPC2, salesprice\n",
    "\n",
    "newLDA1 = a*neighbourhood + b*garagearea\n",
    "newLDA2 = x*neighbourhood + y*garagearea\n",
    "\n",
    "newPC1, salesprice ----> reduced my feature space\n",
    "\n",
    "Linear regre\n",
    "Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input categorical\n",
    "Output categorical\n",
    "chi-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Filter Methods \n",
    "***\n",
    "**Pearson’s Correlation**: It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1. Pearson's correlation is given as:\n",
    "![FS2.png](../images/FS2.png)\n",
    "[Source: An Analysis of Feature Selection Techniques](http://syllabus.cs.manchester.ac.uk/pgt/2017/COMP61011/goodProjects/Shardlow.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6));\n",
    "sns.heatmap(iowa.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Filter Methods\n",
    "***\n",
    "**LDA**: Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.\n",
    "\n",
    "[Source: An Analysis of Feature Selection Techniques](http://syllabus.cs.manchester.ac.uk/pgt/2017/COMP61011/goodProjects/Shardlow.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Filter Methods \n",
    "***\n",
    "**ANOVA**: ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Filter Methods \n",
    "***\n",
    "**Chi-Square**: It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter Methods vulnerability to Multicollinearity\n",
    "***\n",
    "One thing that should be kept in mind is that filter methods do not remove multicollinearity. So, one must deal with multicollinearity of features as well before training models for his/her data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br >\n",
    "\n",
    "## Wrapper Methods\n",
    "***\n",
    "![Wrapper_1.png](../images/Wrapper_1.png)\n",
    "\n",
    "[Source: Analytics Vidya Blog](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iterative fashion\n",
    "scores of your algorithm, internally select the top features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wrapper Methods\n",
    "***\n",
    "* In wrapper methods, we try to use a subset of features and train a model using them.\n",
    "* Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset.\n",
    "* The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "* Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wrapper Methods\n",
    "***\n",
    "**Forward Selection**: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10 features\n",
    "Regression ----> R2\n",
    "\n",
    "1. mean(y)\n",
    "2. F1 + intercept ---> R2\n",
    "3. F1 + F2 + intercept ---> R3\n",
    "4. F1 + F2 + F3 + intercept ---> R4 > R3 > R2 > R1\n",
    "....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wrapper Methods\n",
    "***\n",
    "**Backward Elimination**: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features. This technique is also known as **Recursive Feature Elimination.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Recursive Feature Elimination (RFE)**\n",
    "\n",
    "* The Recursive Feature Elimination (RFE) method works by recursively removing attributes and building a model on those attributes that remain.\n",
    "* It uses an external estimator that assigns weights to features (for example, the coefficients of a linear model) to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. begin with all features\n",
    "LR: coeff\n",
    "trees : feature_score\n",
    "    \n",
    "2. starts to remove the onces which are lowest\n",
    "3. Top K features\n",
    "....\n",
    "left with only K features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Recursive Feature Elimination (RFE)**\n",
    "\n",
    "* It is a greedy optimization algorithm which aims to find the best performing feature subset.\n",
    "\n",
    "* It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. \n",
    "\n",
    "* It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iowa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a base classifier used to evaluate a subset of attributes\n",
    "model = LinearRegression()\n",
    "\n",
    "X, y = iowa.iloc[:,:-1], iowa.iloc[:,-1]\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model, 10)\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Embedded Methods\n",
    "***\n",
    "![Embedded_1.png](../images/Embedded_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods.\n",
    "* Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting.\n",
    "    - Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "    - Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients.\n",
    "* We already have learned how these algorithm works.Other examples of embedded methods are Regularized trees, Memetic algorithm, Random multinomial logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Model Based Feature Selection**\n",
    "\n",
    "* Certain models can be used as feature selection mechanisms because their inner workings involves ordering or ranking of features.\n",
    "* We have already gone through such algorithms:\n",
    "* Brain Teaser:\n",
    "     - What are such algorithms?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model Based Feature Selection**\n",
    "* Here are a few algorithms which help us select feature selection:\n",
    "    - L1 regularized linear regression\n",
    "    - Decision Trees\n",
    "    - Random Forests\n",
    "    - Gradient Boosting Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Difference between Filter and Wrapper methods \n",
    "***\n",
    "The main differences between the filter and wrapper methods for feature selection are:\n",
    "* Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "* Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "* Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Difference between Filter and Wrapper methods \n",
    "***\n",
    "* Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "* Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Good, Moving forward John asked one example with Iowa dataset ...\n",
    "\n",
    "So, Jay give an example with RandomForest ( about the model we will be learning later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before');\n",
    "print(X.shape)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('\\nFeature Importance');\n",
    "print(clf.feature_importances_)\n",
    "mean = clf.feature_importances_.mean()\n",
    "print(\"mean: {}\".format(mean))\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "print('\\nAfter'); \n",
    "print(X_new.shape)\n",
    "clf.feature_importances_[clf.feature_importances_ > mean].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear - biaes\n",
    "rndom, grad - low variance and low bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "250 estmators === decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tree.feature_importances_ for tree in forest.estimators_][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "plt.figure();\n",
    "plt.title(\"Feature importances\");\n",
    "plt.bar(range(X.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\");\n",
    "plt.xticks(range(X.shape[1]), indices);\n",
    "plt.xlim([-1, X.shape[1]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**, or **PCA**, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. It transforms the original variables into a new set of variables such that they are orthogonal (and hence linearly independent) and then ranked according to the variance of data among them. These newly extracted variables are called Principal Components.\n",
    "\n",
    "* Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset.\n",
    "* The second principal component(uncorrelated to the first) tries to explain the remaining variance(not explained by the first).\n",
    "* The third principal component explains the variance not explained by first and second and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS INVOLVED IN PCA\n",
    "\n",
    "* Standardize the data and compute the Covariance Matrix\n",
    "* Compute the Eigenvectors and Eigenvalues of the Covariance Matrix \n",
    "* Sort the eigenvectors according to their eigenvalue (in decreasing order).\n",
    "* Use Eigenvectors corresponding to the (k)largest eigenvalues to reconstruct a large fraction of variance of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "900 features\n",
    "\n",
    "feat1, feat2=2*feat1, feat3, feat4 = 3*feat3\n",
    "\n",
    "PCA will only work, if you have multicollinarity in your IV\n",
    "\n",
    "Target - Cat\n",
    "Feature - Continuous\n",
    "\n",
    "Target - Continuous\n",
    "Feature - Cat\n",
    "\n",
    "Logistic Regression\n",
    "ANOVA\n",
    "\n",
    "2-Sample T-Test: Only for 2 groups\n",
    "ANOVA: More than 2 groups\n",
    "    GDP\n",
    "    mu1 = mu2 = mu3\n",
    "    mu1 /= mu2 /= mu3\n",
    "\n",
    "Salary, Approved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class Activity\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography... \n",
    "\n",
    "Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter. \n",
    "\n",
    "-  Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "-  Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    " -  Attribute Information:\n",
    "\n",
    "    -  The last column of 'spambase.data' denotes whether the e-mail was \n",
    "       considered spam (1) or not (0)\n",
    "    \n",
    "    - 48 attributes are continuous real [0,100] numbers of type `word freq WORD` i.e. percentage of words in the e-mail that         match WORD\n",
    "\n",
    "    - 6 attributes are continuous real [0,100] numbers of type `char freq CHAR` i.e. percentage of characters in the e-mail           that match CHAR\n",
    "    \n",
    "    - 1 attribute is continuous real [1,...] numbers of type `capital run length average` i.e. average length of uninterrupted       sequences of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length longest` i.e. length of longest                   uninterrupted sequence of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length total` i.e. sum of length of uninterrupted       sequences of capital letters in the email\n",
    "\n",
    "    - 1 attribute is nominal {0,1} class  of type spam i.e  denotes whether the e-mail was considered spam (1) or not (0),  \n",
    "\n",
    "- Missing Attribute Values: None\n",
    "\n",
    "- Class Distribution:\n",
    "\tSpam\t  1813  (39.4%)\n",
    "\tNon-Spam  2788  (60.6%)\n",
    "\n",
    "\n",
    "\n",
    "You can read more about dataset [here](https://archive.ics.uci.edu/ml/datasets/spambase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1L emails\n",
    "Vocab = 50k-1L unique words == Feture\n",
    "57 features == KPI\n",
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Drift\n",
    "\n",
    "Train = Distribution\n",
    "Test = Same Distribution\n",
    "\n",
    "Train again\n",
    "1. Constant check of metrics - Dahsboard, reporting\n",
    "\n",
    "Online Machine Learning = Continuously Learning/Adapting with new flow data pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(5, 8, 10, 3, 5) - KPI - Feature Enginerring\n",
    "total(5, 8, 10, 3, 5)\n",
    "\n",
    "mail1 = \n",
    "mail2 = \n",
    "mail3 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# To print multiple outputs together\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Change column display number during print\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2    3     4     5     6     7     8     9    10    11  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  0.00  0.64   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  0.21  0.79   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  0.38  0.45   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31   \n",
       "\n",
       "     12    13    14    15    16    17    18    19    20   21    22    23   24  \\\n",
       "0  0.00  0.00  0.00  0.32  0.00  1.29  1.93  0.00  0.96  0.0  0.00  0.00  0.0   \n",
       "1  0.65  0.21  0.14  0.14  0.07  0.28  3.47  0.00  1.59  0.0  0.43  0.43  0.0   \n",
       "2  0.12  0.00  1.75  0.06  0.06  1.03  1.36  0.32  0.51  0.0  1.16  0.06  0.0   \n",
       "3  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0   \n",
       "4  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0   \n",
       "\n",
       "    25   26   27   28   29   30   31   32   33   34   35    36   37   38  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.07  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "\n",
       "     39   40   41    42   43    44    45   46   47    48     49   50     51  \\\n",
       "0  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.000  0.0  0.778   \n",
       "1  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.132  0.0  0.372   \n",
       "2  0.06  0.0  0.0  0.12  0.0  0.06  0.06  0.0  0.0  0.01  0.143  0.0  0.276   \n",
       "3  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.137  0.0  0.137   \n",
       "4  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.135  0.0  0.135   \n",
       "\n",
       "      52     53     54   55    56  57  \n",
       "0  0.000  0.000  3.756  61   278   1   \n",
       "1  0.180  0.048  5.114  101  1028  1   \n",
       "2  0.184  0.010  9.821  485  2259  1   \n",
       "3  0.000  0.000  3.537  40   191   1   \n",
       "4  0.000  0.000  3.537  40   191   1   "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the Spam data for the mini challenge\n",
    "#Target variable is the 57 column i.e spam, non-spam classes \n",
    "df = pd.read_csv('../data/spambase.data.csv',header=None)\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get an overview of your data by using info() and describe() functions of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      "0     4601 non-null float64\n",
      "1     4601 non-null float64\n",
      "2     4601 non-null float64\n",
      "3     4601 non-null float64\n",
      "4     4601 non-null float64\n",
      "5     4601 non-null float64\n",
      "6     4601 non-null float64\n",
      "7     4601 non-null float64\n",
      "8     4601 non-null float64\n",
      "9     4601 non-null float64\n",
      "10    4601 non-null float64\n",
      "11    4601 non-null float64\n",
      "12    4601 non-null float64\n",
      "13    4601 non-null float64\n",
      "14    4601 non-null float64\n",
      "15    4601 non-null float64\n",
      "16    4601 non-null float64\n",
      "17    4601 non-null float64\n",
      "18    4601 non-null float64\n",
      "19    4601 non-null float64\n",
      "20    4601 non-null float64\n",
      "21    4601 non-null float64\n",
      "22    4601 non-null float64\n",
      "23    4601 non-null float64\n",
      "24    4601 non-null float64\n",
      "25    4601 non-null float64\n",
      "26    4601 non-null float64\n",
      "27    4601 non-null float64\n",
      "28    4601 non-null float64\n",
      "29    4601 non-null float64\n",
      "30    4601 non-null float64\n",
      "31    4601 non-null float64\n",
      "32    4601 non-null float64\n",
      "33    4601 non-null float64\n",
      "34    4601 non-null float64\n",
      "35    4601 non-null float64\n",
      "36    4601 non-null float64\n",
      "37    4601 non-null float64\n",
      "38    4601 non-null float64\n",
      "39    4601 non-null float64\n",
      "40    4601 non-null float64\n",
      "41    4601 non-null float64\n",
      "42    4601 non-null float64\n",
      "43    4601 non-null float64\n",
      "44    4601 non-null float64\n",
      "45    4601 non-null float64\n",
      "46    4601 non-null float64\n",
      "47    4601 non-null float64\n",
      "48    4601 non-null float64\n",
      "49    4601 non-null float64\n",
      "50    4601 non-null float64\n",
      "51    4601 non-null float64\n",
      "52    4601 non-null float64\n",
      "53    4601 non-null float64\n",
      "54    4601 non-null float64\n",
      "55    4601 non-null int64\n",
      "56    4601 non-null int64\n",
      "57    4601 non-null int64\n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.059824</td>\n",
       "      <td>0.541702</td>\n",
       "      <td>0.093930</td>\n",
       "      <td>0.058626</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>0.248848</td>\n",
       "      <td>0.142586</td>\n",
       "      <td>0.184745</td>\n",
       "      <td>1.662100</td>\n",
       "      <td>0.085577</td>\n",
       "      <td>0.809761</td>\n",
       "      <td>0.121202</td>\n",
       "      <td>0.101645</td>\n",
       "      <td>0.094269</td>\n",
       "      <td>0.549504</td>\n",
       "      <td>0.265384</td>\n",
       "      <td>0.767305</td>\n",
       "      <td>0.124845</td>\n",
       "      <td>0.098915</td>\n",
       "      <td>0.102852</td>\n",
       "      <td>0.064753</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>0.097229</td>\n",
       "      <td>0.047835</td>\n",
       "      <td>0.105412</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.136953</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.078629</td>\n",
       "      <td>0.064834</td>\n",
       "      <td>0.043667</td>\n",
       "      <td>0.132339</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>0.079196</td>\n",
       "      <td>0.301224</td>\n",
       "      <td>0.179824</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.031869</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>0.201545</td>\n",
       "      <td>0.861698</td>\n",
       "      <td>0.301036</td>\n",
       "      <td>0.335184</td>\n",
       "      <td>0.258843</td>\n",
       "      <td>0.825792</td>\n",
       "      <td>0.444055</td>\n",
       "      <td>0.531122</td>\n",
       "      <td>1.775481</td>\n",
       "      <td>0.509767</td>\n",
       "      <td>1.200810</td>\n",
       "      <td>1.025756</td>\n",
       "      <td>0.350286</td>\n",
       "      <td>0.442636</td>\n",
       "      <td>1.671349</td>\n",
       "      <td>0.886955</td>\n",
       "      <td>3.367292</td>\n",
       "      <td>0.538576</td>\n",
       "      <td>0.593327</td>\n",
       "      <td>0.456682</td>\n",
       "      <td>0.403393</td>\n",
       "      <td>0.328559</td>\n",
       "      <td>0.555907</td>\n",
       "      <td>0.329445</td>\n",
       "      <td>0.532260</td>\n",
       "      <td>0.402623</td>\n",
       "      <td>0.423451</td>\n",
       "      <td>0.220651</td>\n",
       "      <td>0.434672</td>\n",
       "      <td>0.349916</td>\n",
       "      <td>0.361205</td>\n",
       "      <td>0.766819</td>\n",
       "      <td>0.223812</td>\n",
       "      <td>0.621976</td>\n",
       "      <td>1.011687</td>\n",
       "      <td>0.911119</td>\n",
       "      <td>0.076274</td>\n",
       "      <td>0.285735</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>2.610000</td>\n",
       "      <td>9.670000</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.410000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>9.090000</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>20.830000</td>\n",
       "      <td>16.660000</td>\n",
       "      <td>33.330000</td>\n",
       "      <td>9.090000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.690000</td>\n",
       "      <td>6.890000</td>\n",
       "      <td>8.330000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>22.050000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2            3            4  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.104553     0.213015     0.280656     0.065425     0.312223      \n",
       "std    0.305358     1.290575     0.504143     1.395151     0.672513      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "75%    0.000000     0.000000     0.420000     0.000000     0.380000      \n",
       "max    4.540000     14.280000    5.100000     42.810000    10.000000     \n",
       "\n",
       "                 5            6            7            8            9  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.095901     0.114208     0.105295     0.090067     0.239413      \n",
       "std    0.273824     0.391441     0.401071     0.278616     0.644755      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "75%    0.000000     0.000000     0.000000     0.000000     0.160000      \n",
       "max    5.880000     7.270000     11.110000    5.260000     18.180000     \n",
       "\n",
       "                10           11           12           13           14  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.059824     0.541702     0.093930     0.058626     0.049205      \n",
       "std    0.201545     0.861698     0.301036     0.335184     0.258843      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.100000     0.000000     0.000000     0.000000      \n",
       "75%    0.000000     0.800000     0.000000     0.000000     0.000000      \n",
       "max    2.610000     9.670000     5.550000     10.000000    4.410000      \n",
       "\n",
       "                15           16           17           18           19  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.248848     0.142586     0.184745     1.662100     0.085577      \n",
       "std    0.825792     0.444055     0.531122     1.775481     0.509767      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.000000     0.000000     1.310000     0.000000      \n",
       "75%    0.100000     0.000000     0.000000     2.640000     0.000000      \n",
       "max    20.000000    7.140000     9.090000     18.750000    18.180000     \n",
       "\n",
       "                20           21           22           23           24  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.809761     0.121202     0.101645     0.094269     0.549504      \n",
       "std    1.200810     1.025756     0.350286     0.442636     1.671349      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.220000     0.000000     0.000000     0.000000     0.000000      \n",
       "75%    1.270000     0.000000     0.000000     0.000000     0.000000      \n",
       "max    11.110000    17.100000    5.450000     12.500000    20.830000     \n",
       "\n",
       "                25           26           27           28           29  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.265384     0.767305     0.124845     0.098915     0.102852      \n",
       "std    0.886955     3.367292     0.538576     0.593327     0.456682      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "75%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "max    16.660000    33.330000    9.090000     14.280000    5.880000      \n",
       "\n",
       "                30           31           32           33           34  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.064753     0.047048     0.097229     0.047835     0.105412      \n",
       "std    0.403393     0.328559     0.555907     0.329445     0.532260      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "75%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "max    12.500000    4.760000     18.180000    4.760000     20.000000     \n",
       "\n",
       "                35           36           37           38           39  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.097477     0.136953     0.013201     0.078629     0.064834      \n",
       "std    0.402623     0.423451     0.220651     0.434672     0.349916      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "75%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "max    7.690000     6.890000     8.330000     11.110000    4.760000      \n",
       "\n",
       "                40           41           42           43           44  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.043667     0.132339     0.046099     0.079196     0.301224      \n",
       "std    0.361205     0.766819     0.223812     0.621976     1.011687      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "75%    0.000000     0.000000     0.000000     0.000000     0.110000      \n",
       "max    7.140000     14.280000    3.570000     20.000000    21.420000     \n",
       "\n",
       "                45           46           47           48           49  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.179824     0.005444     0.031869     0.038575     0.139030      \n",
       "std    0.911119     0.076274     0.285735     0.243471     0.270355      \n",
       "min    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     0.000000      \n",
       "50%    0.000000     0.000000     0.000000     0.000000     0.065000      \n",
       "75%    0.000000     0.000000     0.000000     0.000000     0.188000      \n",
       "max    22.050000    2.170000     10.000000    4.385000     9.752000      \n",
       "\n",
       "                50           51           52           53           54  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean   0.016976     0.269071     0.075811     0.044238     5.191515      \n",
       "std    0.109394     0.815672     0.245882     0.429342     31.729449     \n",
       "min    0.000000     0.000000     0.000000     0.000000     1.000000      \n",
       "25%    0.000000     0.000000     0.000000     0.000000     1.588000      \n",
       "50%    0.000000     0.000000     0.000000     0.000000     2.276000      \n",
       "75%    0.000000     0.315000     0.052000     0.000000     3.706000      \n",
       "max    4.081000     32.478000    6.003000     19.829000    1102.500000   \n",
       "\n",
       "                55            56           57  \n",
       "count  4601.000000  4601.000000   4601.000000  \n",
       "mean   52.172789    283.289285    0.394045     \n",
       "std    194.891310   606.347851    0.488698     \n",
       "min    1.000000     1.000000      0.000000     \n",
       "25%    6.000000     35.000000     0.000000     \n",
       "50%    15.000000    95.000000     0.000000     \n",
       "75%    43.000000    266.000000    1.000000     \n",
       "max    9989.000000  15841.000000  1.000000     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the data\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data into train and test set, standardise your date and fit the base logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dividing the dataset set in train and test set and apply base logistic model\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)\n",
    "\n",
    "sc = StandardScaler() \n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test) \n",
    "\n",
    "lr = LogisticRegression(random_state=101)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9094858797972484\n",
      "Confusion Matrix: \n",
      " [[773  49]\n",
      " [ 76 483]]\n",
      "========================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93       822\n",
      "           1       0.91      0.86      0.89       559\n",
      "\n",
      "    accuracy                           0.91      1381\n",
      "   macro avg       0.91      0.90      0.91      1381\n",
      "weighted avg       0.91      0.91      0.91      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix.\n",
    "print(\"Accuracy on test data:\", lr.score(X_test, y_test))\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test,y_pred))\n",
    "print(\"==\"*20)\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Copy dataset df into df1 variable and apply correlation on df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2    3     4     5     6     7     8     9    10    11  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  0.00  0.64   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  0.21  0.79   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  0.38  0.45   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31   \n",
       "\n",
       "     12    13    14    15    16    17    18    19    20   21    22    23   24  \\\n",
       "0  0.00  0.00  0.00  0.32  0.00  1.29  1.93  0.00  0.96  0.0  0.00  0.00  0.0   \n",
       "1  0.65  0.21  0.14  0.14  0.07  0.28  3.47  0.00  1.59  0.0  0.43  0.43  0.0   \n",
       "2  0.12  0.00  1.75  0.06  0.06  1.03  1.36  0.32  0.51  0.0  1.16  0.06  0.0   \n",
       "3  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0   \n",
       "4  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0   \n",
       "\n",
       "    25   26   27   28   29   30   31   32   33   34   35    36   37   38  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.07  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "\n",
       "     39   40   41    42   43    44    45   46   47    48     49   50     51  \\\n",
       "0  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.000  0.0  0.778   \n",
       "1  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.132  0.0  0.372   \n",
       "2  0.06  0.0  0.0  0.12  0.0  0.06  0.06  0.0  0.0  0.01  0.143  0.0  0.276   \n",
       "3  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.137  0.0  0.137   \n",
       "4  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.135  0.0  0.135   \n",
       "\n",
       "      52     53     54   55    56  57  \n",
       "0  0.000  0.000  3.756  61   278   1   \n",
       "1  0.180  0.048  5.114  101  1028  1   \n",
       "2  0.184  0.010  9.821  485  2259  1   \n",
       "3  0.000  0.000  3.537  40   191   1   \n",
       "4  0.000  0.000  3.537  40   191   1   "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy df in new variable df1\n",
    "df1 = df.copy()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. As we have learned  one of the assumptions of Logistic Regression model is that the independent features should not be correlated to each other(i.e Multicollinearity), So we have to find the features that have a correlation higher that 0.75 and remove the same so that the assumption for logistic regression model is satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.013273</td>\n",
       "      <td>0.023119</td>\n",
       "      <td>0.059674</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.041198</td>\n",
       "      <td>0.188459</td>\n",
       "      <td>0.105801</td>\n",
       "      <td>0.066438</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.028439</td>\n",
       "      <td>0.059386</td>\n",
       "      <td>0.081928</td>\n",
       "      <td>0.053324</td>\n",
       "      <td>0.128243</td>\n",
       "      <td>0.021295</td>\n",
       "      <td>0.197049</td>\n",
       "      <td>0.024349</td>\n",
       "      <td>0.134072</td>\n",
       "      <td>0.188155</td>\n",
       "      <td>0.072504</td>\n",
       "      <td>0.061686</td>\n",
       "      <td>0.066424</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.052799</td>\n",
       "      <td>0.039066</td>\n",
       "      <td>0.032058</td>\n",
       "      <td>0.041014</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.044954</td>\n",
       "      <td>0.054673</td>\n",
       "      <td>0.057312</td>\n",
       "      <td>0.007960</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>0.036095</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.024292</td>\n",
       "      <td>0.022116</td>\n",
       "      <td>0.037105</td>\n",
       "      <td>0.034056</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.058292</td>\n",
       "      <td>0.117419</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>0.044491</td>\n",
       "      <td>0.061382</td>\n",
       "      <td>0.089165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.023760</td>\n",
       "      <td>0.024840</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.040398</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.009206</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.055476</td>\n",
       "      <td>0.015806</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.020502</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>0.038211</td>\n",
       "      <td>0.030307</td>\n",
       "      <td>0.029221</td>\n",
       "      <td>0.021940</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.018097</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.024903</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.019124</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>0.015420</td>\n",
       "      <td>0.025177</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.019739</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.009818</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>0.049837</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.022680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.087564</td>\n",
       "      <td>0.036677</td>\n",
       "      <td>0.012003</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.032075</td>\n",
       "      <td>0.048254</td>\n",
       "      <td>0.083210</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.008552</td>\n",
       "      <td>0.122113</td>\n",
       "      <td>0.063906</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.121923</td>\n",
       "      <td>0.139329</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>0.156651</td>\n",
       "      <td>0.035681</td>\n",
       "      <td>0.123671</td>\n",
       "      <td>0.041145</td>\n",
       "      <td>0.087924</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>0.108886</td>\n",
       "      <td>0.050648</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.038927</td>\n",
       "      <td>0.061870</td>\n",
       "      <td>0.054759</td>\n",
       "      <td>0.061706</td>\n",
       "      <td>0.048335</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>0.067015</td>\n",
       "      <td>0.032407</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>0.047066</td>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>0.044325</td>\n",
       "      <td>0.053464</td>\n",
       "      <td>0.050664</td>\n",
       "      <td>0.056655</td>\n",
       "      <td>0.029339</td>\n",
       "      <td>0.026344</td>\n",
       "      <td>0.033213</td>\n",
       "      <td>0.016495</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.087618</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.097398</td>\n",
       "      <td>0.107463</td>\n",
       "      <td>0.070114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013273</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.012976</td>\n",
       "      <td>0.019221</td>\n",
       "      <td>0.013199</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.028102</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>0.015181</td>\n",
       "      <td>0.013708</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>0.007761</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.021369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023119</td>\n",
       "      <td>0.023760</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.020823</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>0.068382</td>\n",
       "      <td>0.066788</td>\n",
       "      <td>0.031126</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.056177</td>\n",
       "      <td>0.083024</td>\n",
       "      <td>0.143443</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>0.098510</td>\n",
       "      <td>0.031526</td>\n",
       "      <td>0.136605</td>\n",
       "      <td>0.020207</td>\n",
       "      <td>0.070037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.072502</td>\n",
       "      <td>0.075456</td>\n",
       "      <td>0.088011</td>\n",
       "      <td>0.061501</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>0.052066</td>\n",
       "      <td>0.042535</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>0.031998</td>\n",
       "      <td>0.026960</td>\n",
       "      <td>0.049732</td>\n",
       "      <td>0.048844</td>\n",
       "      <td>0.072599</td>\n",
       "      <td>0.130812</td>\n",
       "      <td>0.042044</td>\n",
       "      <td>0.021442</td>\n",
       "      <td>0.047505</td>\n",
       "      <td>0.115041</td>\n",
       "      <td>0.048879</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.042336</td>\n",
       "      <td>0.077986</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.032759</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.059674</td>\n",
       "      <td>0.024840</td>\n",
       "      <td>0.087564</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>0.079561</td>\n",
       "      <td>0.117438</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.077631</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.173066</td>\n",
       "      <td>0.019865</td>\n",
       "      <td>0.064137</td>\n",
       "      <td>0.078350</td>\n",
       "      <td>0.095505</td>\n",
       "      <td>0.058979</td>\n",
       "      <td>0.106833</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.211455</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>0.084402</td>\n",
       "      <td>0.087271</td>\n",
       "      <td>0.069051</td>\n",
       "      <td>0.066223</td>\n",
       "      <td>0.048673</td>\n",
       "      <td>0.048127</td>\n",
       "      <td>0.046383</td>\n",
       "      <td>0.036835</td>\n",
       "      <td>0.034164</td>\n",
       "      <td>0.037315</td>\n",
       "      <td>0.054315</td>\n",
       "      <td>0.052819</td>\n",
       "      <td>0.057465</td>\n",
       "      <td>0.017918</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.029866</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.054812</td>\n",
       "      <td>0.030616</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.053637</td>\n",
       "      <td>0.033046</td>\n",
       "      <td>0.014343</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.015133</td>\n",
       "      <td>0.065043</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.090172</td>\n",
       "      <td>0.082089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.036677</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044545</td>\n",
       "      <td>0.050786</td>\n",
       "      <td>0.056809</td>\n",
       "      <td>0.159578</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.042904</td>\n",
       "      <td>0.128436</td>\n",
       "      <td>0.187981</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.111792</td>\n",
       "      <td>0.046134</td>\n",
       "      <td>0.130794</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.064795</td>\n",
       "      <td>0.030575</td>\n",
       "      <td>0.089494</td>\n",
       "      <td>0.080330</td>\n",
       "      <td>0.065893</td>\n",
       "      <td>0.066947</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>0.046280</td>\n",
       "      <td>0.040538</td>\n",
       "      <td>0.041372</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>0.053978</td>\n",
       "      <td>0.052035</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>0.046978</td>\n",
       "      <td>0.022121</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.049664</td>\n",
       "      <td>0.049079</td>\n",
       "      <td>0.034461</td>\n",
       "      <td>0.050811</td>\n",
       "      <td>0.056166</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>0.031408</td>\n",
       "      <td>0.033089</td>\n",
       "      <td>0.051885</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>0.070127</td>\n",
       "      <td>0.046612</td>\n",
       "      <td>0.041565</td>\n",
       "      <td>0.059677</td>\n",
       "      <td>0.008344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.012003</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.079561</td>\n",
       "      <td>0.044545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>0.083129</td>\n",
       "      <td>0.128495</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.026274</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.072782</td>\n",
       "      <td>0.051115</td>\n",
       "      <td>0.216422</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.020641</td>\n",
       "      <td>0.109163</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>0.089226</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>0.057189</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>0.037047</td>\n",
       "      <td>0.043405</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>0.034276</td>\n",
       "      <td>0.039220</td>\n",
       "      <td>0.034811</td>\n",
       "      <td>0.035174</td>\n",
       "      <td>0.033747</td>\n",
       "      <td>0.017466</td>\n",
       "      <td>0.012119</td>\n",
       "      <td>0.030392</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>0.043626</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.037916</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.027432</td>\n",
       "      <td>0.032494</td>\n",
       "      <td>0.019548</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.037575</td>\n",
       "      <td>0.040252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.020823</td>\n",
       "      <td>0.117438</td>\n",
       "      <td>0.050786</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.137760</td>\n",
       "      <td>0.030344</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.066840</td>\n",
       "      <td>0.238436</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.158390</td>\n",
       "      <td>0.098804</td>\n",
       "      <td>0.039017</td>\n",
       "      <td>0.123217</td>\n",
       "      <td>0.159112</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>0.126800</td>\n",
       "      <td>0.099461</td>\n",
       "      <td>0.069931</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>0.064608</td>\n",
       "      <td>0.056764</td>\n",
       "      <td>0.044840</td>\n",
       "      <td>0.043643</td>\n",
       "      <td>0.040158</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.033601</td>\n",
       "      <td>0.041847</td>\n",
       "      <td>0.056270</td>\n",
       "      <td>0.033244</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.040844</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0.035177</td>\n",
       "      <td>0.048223</td>\n",
       "      <td>0.034190</td>\n",
       "      <td>0.035159</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.056817</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.026017</td>\n",
       "      <td>0.014646</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.043639</td>\n",
       "      <td>0.149365</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.111308</td>\n",
       "      <td>0.189247</td>\n",
       "      <td>0.248724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.041198</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>0.032075</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.056809</td>\n",
       "      <td>0.083129</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125319</td>\n",
       "      <td>0.071157</td>\n",
       "      <td>0.045737</td>\n",
       "      <td>0.017901</td>\n",
       "      <td>0.160543</td>\n",
       "      <td>0.025601</td>\n",
       "      <td>0.081363</td>\n",
       "      <td>0.035977</td>\n",
       "      <td>0.093509</td>\n",
       "      <td>0.030859</td>\n",
       "      <td>0.098072</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.096809</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.033534</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.067817</td>\n",
       "      <td>0.019356</td>\n",
       "      <td>0.026903</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.014434</td>\n",
       "      <td>0.020092</td>\n",
       "      <td>0.016955</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.017950</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.026654</td>\n",
       "      <td>0.032065</td>\n",
       "      <td>0.030326</td>\n",
       "      <td>0.015546</td>\n",
       "      <td>0.016842</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.036737</td>\n",
       "      <td>0.075786</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.073677</td>\n",
       "      <td>0.103308</td>\n",
       "      <td>0.087273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.188459</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.048254</td>\n",
       "      <td>0.012976</td>\n",
       "      <td>0.068382</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.159578</td>\n",
       "      <td>0.128495</td>\n",
       "      <td>0.137760</td>\n",
       "      <td>0.125319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.118259</td>\n",
       "      <td>0.049203</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>0.064718</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.174198</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>0.141780</td>\n",
       "      <td>0.155769</td>\n",
       "      <td>0.294826</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.106324</td>\n",
       "      <td>0.056069</td>\n",
       "      <td>0.075640</td>\n",
       "      <td>0.076468</td>\n",
       "      <td>0.065010</td>\n",
       "      <td>0.059017</td>\n",
       "      <td>0.045916</td>\n",
       "      <td>0.053411</td>\n",
       "      <td>0.032690</td>\n",
       "      <td>0.036749</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>0.036384</td>\n",
       "      <td>0.046675</td>\n",
       "      <td>0.050373</td>\n",
       "      <td>0.032310</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.042482</td>\n",
       "      <td>0.024440</td>\n",
       "      <td>0.034654</td>\n",
       "      <td>0.043150</td>\n",
       "      <td>0.034560</td>\n",
       "      <td>0.036533</td>\n",
       "      <td>0.064104</td>\n",
       "      <td>0.050896</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>0.025183</td>\n",
       "      <td>0.024992</td>\n",
       "      <td>0.070227</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.029258</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>0.115055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.105801</td>\n",
       "      <td>0.040398</td>\n",
       "      <td>0.083210</td>\n",
       "      <td>0.019221</td>\n",
       "      <td>0.066788</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.030344</td>\n",
       "      <td>0.071157</td>\n",
       "      <td>0.118259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>0.028699</td>\n",
       "      <td>0.055409</td>\n",
       "      <td>0.012901</td>\n",
       "      <td>0.088045</td>\n",
       "      <td>0.012239</td>\n",
       "      <td>0.105631</td>\n",
       "      <td>0.045031</td>\n",
       "      <td>0.016007</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.119059</td>\n",
       "      <td>0.039711</td>\n",
       "      <td>0.101684</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.035124</td>\n",
       "      <td>0.055837</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.056589</td>\n",
       "      <td>0.048154</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>0.026329</td>\n",
       "      <td>0.025483</td>\n",
       "      <td>0.018725</td>\n",
       "      <td>0.036452</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>0.111335</td>\n",
       "      <td>0.024628</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.088843</td>\n",
       "      <td>0.071947</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>0.044966</td>\n",
       "      <td>0.013369</td>\n",
       "      <td>0.016723</td>\n",
       "      <td>0.030445</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.020076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.066438</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.013199</td>\n",
       "      <td>0.031126</td>\n",
       "      <td>0.077631</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.026274</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.045737</td>\n",
       "      <td>0.049203</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063683</td>\n",
       "      <td>0.079730</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.049218</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>0.118721</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.049089</td>\n",
       "      <td>0.029978</td>\n",
       "      <td>0.119115</td>\n",
       "      <td>0.078374</td>\n",
       "      <td>0.059361</td>\n",
       "      <td>0.067375</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>0.060479</td>\n",
       "      <td>0.027388</td>\n",
       "      <td>0.052332</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.031850</td>\n",
       "      <td>0.046344</td>\n",
       "      <td>0.031919</td>\n",
       "      <td>0.048982</td>\n",
       "      <td>0.046861</td>\n",
       "      <td>0.042367</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.018885</td>\n",
       "      <td>0.038718</td>\n",
       "      <td>0.012051</td>\n",
       "      <td>0.024445</td>\n",
       "      <td>0.042278</td>\n",
       "      <td>0.021946</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.019746</td>\n",
       "      <td>0.023445</td>\n",
       "      <td>0.051151</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>0.040737</td>\n",
       "      <td>0.205905</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.105150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.009206</td>\n",
       "      <td>0.008552</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.066840</td>\n",
       "      <td>0.017901</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>0.063683</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.028065</td>\n",
       "      <td>0.018087</td>\n",
       "      <td>0.032940</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>0.019458</td>\n",
       "      <td>0.044354</td>\n",
       "      <td>0.031922</td>\n",
       "      <td>0.035957</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.028039</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>0.028975</td>\n",
       "      <td>0.018236</td>\n",
       "      <td>0.018514</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.024597</td>\n",
       "      <td>0.008924</td>\n",
       "      <td>0.030316</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.026516</td>\n",
       "      <td>0.024623</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.012522</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.020994</td>\n",
       "      <td>0.049647</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.019045</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.014349</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.080953</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.060993</td>\n",
       "      <td>0.169257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.028439</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.122113</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.056177</td>\n",
       "      <td>0.173066</td>\n",
       "      <td>0.042904</td>\n",
       "      <td>0.072782</td>\n",
       "      <td>0.238436</td>\n",
       "      <td>0.160543</td>\n",
       "      <td>0.064718</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>0.079730</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.254489</td>\n",
       "      <td>0.040029</td>\n",
       "      <td>0.016516</td>\n",
       "      <td>0.073393</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.369588</td>\n",
       "      <td>0.034731</td>\n",
       "      <td>0.048988</td>\n",
       "      <td>0.031237</td>\n",
       "      <td>0.041069</td>\n",
       "      <td>0.036775</td>\n",
       "      <td>0.028513</td>\n",
       "      <td>0.035377</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.022921</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.023313</td>\n",
       "      <td>0.025376</td>\n",
       "      <td>0.034381</td>\n",
       "      <td>0.036186</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>0.046247</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>0.031883</td>\n",
       "      <td>0.049018</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>0.038931</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>0.018277</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.018607</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.151626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.059386</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.063906</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.083024</td>\n",
       "      <td>0.019865</td>\n",
       "      <td>0.128436</td>\n",
       "      <td>0.051115</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.025601</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.028699</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055753</td>\n",
       "      <td>0.072007</td>\n",
       "      <td>0.081897</td>\n",
       "      <td>0.027665</td>\n",
       "      <td>0.102815</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.096812</td>\n",
       "      <td>0.090139</td>\n",
       "      <td>0.079237</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.063569</td>\n",
       "      <td>0.046255</td>\n",
       "      <td>0.058076</td>\n",
       "      <td>0.043714</td>\n",
       "      <td>0.039382</td>\n",
       "      <td>0.043468</td>\n",
       "      <td>0.038832</td>\n",
       "      <td>0.055825</td>\n",
       "      <td>0.060342</td>\n",
       "      <td>0.060394</td>\n",
       "      <td>0.017720</td>\n",
       "      <td>0.035150</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.028342</td>\n",
       "      <td>0.037275</td>\n",
       "      <td>0.047430</td>\n",
       "      <td>0.031556</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.047441</td>\n",
       "      <td>0.017837</td>\n",
       "      <td>0.028084</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>0.046578</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.104261</td>\n",
       "      <td>0.049953</td>\n",
       "      <td>0.035534</td>\n",
       "      <td>0.015036</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.003007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.081928</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.143443</td>\n",
       "      <td>0.064137</td>\n",
       "      <td>0.187981</td>\n",
       "      <td>0.216422</td>\n",
       "      <td>0.158390</td>\n",
       "      <td>0.081363</td>\n",
       "      <td>0.174198</td>\n",
       "      <td>0.055409</td>\n",
       "      <td>0.049218</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.055753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>0.084983</td>\n",
       "      <td>0.190948</td>\n",
       "      <td>0.221831</td>\n",
       "      <td>0.021045</td>\n",
       "      <td>0.097284</td>\n",
       "      <td>0.044865</td>\n",
       "      <td>0.052977</td>\n",
       "      <td>0.076310</td>\n",
       "      <td>0.069724</td>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.045565</td>\n",
       "      <td>0.036064</td>\n",
       "      <td>0.041464</td>\n",
       "      <td>0.042402</td>\n",
       "      <td>0.040938</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>0.055240</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.049606</td>\n",
       "      <td>0.015373</td>\n",
       "      <td>0.042276</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.036323</td>\n",
       "      <td>0.040693</td>\n",
       "      <td>0.052646</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.059565</td>\n",
       "      <td>0.057915</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.029947</td>\n",
       "      <td>0.031542</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>0.077049</td>\n",
       "      <td>0.098323</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.038126</td>\n",
       "      <td>0.062672</td>\n",
       "      <td>0.064261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.053324</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.121923</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>0.078350</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.098804</td>\n",
       "      <td>0.035977</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>0.012901</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>0.028065</td>\n",
       "      <td>0.254489</td>\n",
       "      <td>0.072007</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094580</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>0.137954</td>\n",
       "      <td>0.026496</td>\n",
       "      <td>0.092498</td>\n",
       "      <td>0.092390</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.067658</td>\n",
       "      <td>0.037933</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>0.027698</td>\n",
       "      <td>0.029224</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.031627</td>\n",
       "      <td>0.043512</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.024925</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.029480</td>\n",
       "      <td>0.047379</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>0.032629</td>\n",
       "      <td>0.047838</td>\n",
       "      <td>0.039857</td>\n",
       "      <td>0.021317</td>\n",
       "      <td>0.017173</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.039350</td>\n",
       "      <td>0.063872</td>\n",
       "      <td>0.020978</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>0.046364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.128243</td>\n",
       "      <td>0.055476</td>\n",
       "      <td>0.139329</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>0.098510</td>\n",
       "      <td>0.095505</td>\n",
       "      <td>0.111792</td>\n",
       "      <td>0.020641</td>\n",
       "      <td>0.039017</td>\n",
       "      <td>0.093509</td>\n",
       "      <td>0.141780</td>\n",
       "      <td>0.088045</td>\n",
       "      <td>0.118721</td>\n",
       "      <td>0.018087</td>\n",
       "      <td>0.040029</td>\n",
       "      <td>0.081897</td>\n",
       "      <td>0.084983</td>\n",
       "      <td>0.094580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>0.304986</td>\n",
       "      <td>0.023681</td>\n",
       "      <td>0.115905</td>\n",
       "      <td>0.184075</td>\n",
       "      <td>0.197683</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.154758</td>\n",
       "      <td>0.116213</td>\n",
       "      <td>0.086828</td>\n",
       "      <td>0.107662</td>\n",
       "      <td>0.089327</td>\n",
       "      <td>0.075169</td>\n",
       "      <td>0.098570</td>\n",
       "      <td>0.075039</td>\n",
       "      <td>0.089516</td>\n",
       "      <td>0.134514</td>\n",
       "      <td>0.134785</td>\n",
       "      <td>0.032566</td>\n",
       "      <td>0.039316</td>\n",
       "      <td>0.060591</td>\n",
       "      <td>0.050328</td>\n",
       "      <td>0.087965</td>\n",
       "      <td>0.051636</td>\n",
       "      <td>0.065022</td>\n",
       "      <td>0.098270</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.040015</td>\n",
       "      <td>0.044314</td>\n",
       "      <td>0.128882</td>\n",
       "      <td>0.063826</td>\n",
       "      <td>0.153381</td>\n",
       "      <td>0.091470</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.030592</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>0.007307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.021295</td>\n",
       "      <td>0.015806</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.031526</td>\n",
       "      <td>0.058979</td>\n",
       "      <td>0.046134</td>\n",
       "      <td>0.109163</td>\n",
       "      <td>0.123217</td>\n",
       "      <td>0.030859</td>\n",
       "      <td>0.155769</td>\n",
       "      <td>0.012239</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.032940</td>\n",
       "      <td>0.016516</td>\n",
       "      <td>0.027665</td>\n",
       "      <td>0.190948</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133062</td>\n",
       "      <td>0.033884</td>\n",
       "      <td>0.041077</td>\n",
       "      <td>0.060560</td>\n",
       "      <td>0.048038</td>\n",
       "      <td>0.049375</td>\n",
       "      <td>0.037469</td>\n",
       "      <td>0.037961</td>\n",
       "      <td>0.027911</td>\n",
       "      <td>0.037214</td>\n",
       "      <td>0.026591</td>\n",
       "      <td>0.023471</td>\n",
       "      <td>0.027645</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.031415</td>\n",
       "      <td>0.037741</td>\n",
       "      <td>0.041077</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.028512</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.013478</td>\n",
       "      <td>0.043708</td>\n",
       "      <td>0.030897</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.020851</td>\n",
       "      <td>0.021431</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>0.048350</td>\n",
       "      <td>0.034948</td>\n",
       "      <td>0.007214</td>\n",
       "      <td>0.067140</td>\n",
       "      <td>0.099463</td>\n",
       "      <td>0.075751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.197049</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>0.156651</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.136605</td>\n",
       "      <td>0.106833</td>\n",
       "      <td>0.130794</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>0.159112</td>\n",
       "      <td>0.098072</td>\n",
       "      <td>0.294826</td>\n",
       "      <td>0.105631</td>\n",
       "      <td>0.049089</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>0.073393</td>\n",
       "      <td>0.102815</td>\n",
       "      <td>0.221831</td>\n",
       "      <td>0.137954</td>\n",
       "      <td>0.304986</td>\n",
       "      <td>0.133062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021361</td>\n",
       "      <td>0.118533</td>\n",
       "      <td>0.165361</td>\n",
       "      <td>0.154633</td>\n",
       "      <td>0.135311</td>\n",
       "      <td>0.126247</td>\n",
       "      <td>0.095474</td>\n",
       "      <td>0.065026</td>\n",
       "      <td>0.091213</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.080787</td>\n",
       "      <td>0.050690</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.105648</td>\n",
       "      <td>0.114988</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>0.063244</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.058967</td>\n",
       "      <td>0.081266</td>\n",
       "      <td>0.050861</td>\n",
       "      <td>0.059901</td>\n",
       "      <td>0.043310</td>\n",
       "      <td>0.081662</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>0.050951</td>\n",
       "      <td>0.058660</td>\n",
       "      <td>0.085181</td>\n",
       "      <td>0.045469</td>\n",
       "      <td>0.084017</td>\n",
       "      <td>0.141649</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.041066</td>\n",
       "      <td>0.085321</td>\n",
       "      <td>0.051797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.024349</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.035681</td>\n",
       "      <td>0.028102</td>\n",
       "      <td>0.020207</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.045031</td>\n",
       "      <td>0.029978</td>\n",
       "      <td>0.019458</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>0.021045</td>\n",
       "      <td>0.026496</td>\n",
       "      <td>0.023681</td>\n",
       "      <td>0.033884</td>\n",
       "      <td>0.021361</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>0.010915</td>\n",
       "      <td>0.038675</td>\n",
       "      <td>0.035020</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>0.027391</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.026613</td>\n",
       "      <td>0.018971</td>\n",
       "      <td>0.016924</td>\n",
       "      <td>0.019311</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.023329</td>\n",
       "      <td>0.027201</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.018842</td>\n",
       "      <td>0.021537</td>\n",
       "      <td>0.014251</td>\n",
       "      <td>0.020389</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.032059</td>\n",
       "      <td>0.020849</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>0.416608</td>\n",
       "      <td>0.046244</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.184428</td>\n",
       "      <td>0.021497</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.103954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.134072</td>\n",
       "      <td>0.020502</td>\n",
       "      <td>0.123671</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.070037</td>\n",
       "      <td>0.211455</td>\n",
       "      <td>0.064795</td>\n",
       "      <td>0.089226</td>\n",
       "      <td>0.126800</td>\n",
       "      <td>0.096809</td>\n",
       "      <td>0.106324</td>\n",
       "      <td>0.016007</td>\n",
       "      <td>0.119115</td>\n",
       "      <td>0.044354</td>\n",
       "      <td>0.369588</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.097284</td>\n",
       "      <td>0.092498</td>\n",
       "      <td>0.115905</td>\n",
       "      <td>0.041077</td>\n",
       "      <td>0.118533</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052587</td>\n",
       "      <td>0.086034</td>\n",
       "      <td>0.081171</td>\n",
       "      <td>0.066007</td>\n",
       "      <td>0.066752</td>\n",
       "      <td>0.046636</td>\n",
       "      <td>0.062341</td>\n",
       "      <td>0.046049</td>\n",
       "      <td>0.041561</td>\n",
       "      <td>0.048133</td>\n",
       "      <td>0.041840</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>0.061425</td>\n",
       "      <td>0.069769</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.038280</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.034898</td>\n",
       "      <td>0.049861</td>\n",
       "      <td>0.041894</td>\n",
       "      <td>0.034337</td>\n",
       "      <td>0.053469</td>\n",
       "      <td>0.053271</td>\n",
       "      <td>0.017122</td>\n",
       "      <td>0.030155</td>\n",
       "      <td>0.027362</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.070103</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.123036</td>\n",
       "      <td>0.165977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.188155</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.041145</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>0.030575</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>0.099461</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.056069</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.078374</td>\n",
       "      <td>0.031922</td>\n",
       "      <td>0.034731</td>\n",
       "      <td>0.096812</td>\n",
       "      <td>0.044865</td>\n",
       "      <td>0.092390</td>\n",
       "      <td>0.184075</td>\n",
       "      <td>0.060560</td>\n",
       "      <td>0.165361</td>\n",
       "      <td>0.010915</td>\n",
       "      <td>0.052587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067306</td>\n",
       "      <td>0.061840</td>\n",
       "      <td>0.047471</td>\n",
       "      <td>0.047839</td>\n",
       "      <td>0.035461</td>\n",
       "      <td>0.046977</td>\n",
       "      <td>0.033128</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.030398</td>\n",
       "      <td>0.038791</td>\n",
       "      <td>0.046050</td>\n",
       "      <td>0.055013</td>\n",
       "      <td>0.007913</td>\n",
       "      <td>0.033614</td>\n",
       "      <td>0.022946</td>\n",
       "      <td>0.025703</td>\n",
       "      <td>0.034146</td>\n",
       "      <td>0.039172</td>\n",
       "      <td>0.023522</td>\n",
       "      <td>0.045922</td>\n",
       "      <td>0.031401</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.019171</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.020798</td>\n",
       "      <td>0.051076</td>\n",
       "      <td>0.104691</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.044870</td>\n",
       "      <td>0.080993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.072504</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>0.087924</td>\n",
       "      <td>0.015181</td>\n",
       "      <td>0.072502</td>\n",
       "      <td>0.084402</td>\n",
       "      <td>0.089494</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>0.069931</td>\n",
       "      <td>0.033534</td>\n",
       "      <td>0.075640</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>0.059361</td>\n",
       "      <td>0.035957</td>\n",
       "      <td>0.048988</td>\n",
       "      <td>0.090139</td>\n",
       "      <td>0.052977</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.197683</td>\n",
       "      <td>0.048038</td>\n",
       "      <td>0.154633</td>\n",
       "      <td>0.038675</td>\n",
       "      <td>0.086034</td>\n",
       "      <td>0.067306</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.517293</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.335034</td>\n",
       "      <td>0.215256</td>\n",
       "      <td>0.448525</td>\n",
       "      <td>0.346018</td>\n",
       "      <td>0.366217</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.364833</td>\n",
       "      <td>0.314544</td>\n",
       "      <td>0.386132</td>\n",
       "      <td>0.129842</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.325930</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.114237</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.049676</td>\n",
       "      <td>0.045916</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.029181</td>\n",
       "      <td>0.136979</td>\n",
       "      <td>0.039723</td>\n",
       "      <td>0.090862</td>\n",
       "      <td>0.086634</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.051206</td>\n",
       "      <td>0.043267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.061686</td>\n",
       "      <td>0.038211</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>0.013708</td>\n",
       "      <td>0.075456</td>\n",
       "      <td>0.087271</td>\n",
       "      <td>0.080330</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.076468</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.067375</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.031237</td>\n",
       "      <td>0.079237</td>\n",
       "      <td>0.076310</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.049375</td>\n",
       "      <td>0.135311</td>\n",
       "      <td>0.035020</td>\n",
       "      <td>0.081171</td>\n",
       "      <td>0.061840</td>\n",
       "      <td>0.517293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010728</td>\n",
       "      <td>0.373212</td>\n",
       "      <td>0.222459</td>\n",
       "      <td>0.389279</td>\n",
       "      <td>0.339387</td>\n",
       "      <td>0.362120</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.361089</td>\n",
       "      <td>0.328805</td>\n",
       "      <td>0.353362</td>\n",
       "      <td>0.159838</td>\n",
       "      <td>0.021865</td>\n",
       "      <td>0.055968</td>\n",
       "      <td>0.320140</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.058059</td>\n",
       "      <td>0.126805</td>\n",
       "      <td>0.015347</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>0.037212</td>\n",
       "      <td>0.033909</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>0.144771</td>\n",
       "      <td>0.064349</td>\n",
       "      <td>0.078367</td>\n",
       "      <td>0.081198</td>\n",
       "      <td>0.020691</td>\n",
       "      <td>0.024234</td>\n",
       "      <td>0.051806</td>\n",
       "      <td>0.059601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.066424</td>\n",
       "      <td>0.030307</td>\n",
       "      <td>0.108886</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.088011</td>\n",
       "      <td>0.069051</td>\n",
       "      <td>0.065893</td>\n",
       "      <td>0.057189</td>\n",
       "      <td>0.064608</td>\n",
       "      <td>0.067817</td>\n",
       "      <td>0.065010</td>\n",
       "      <td>0.119059</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>0.028039</td>\n",
       "      <td>0.041069</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.069724</td>\n",
       "      <td>0.067658</td>\n",
       "      <td>0.154758</td>\n",
       "      <td>0.037469</td>\n",
       "      <td>0.126247</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>0.066007</td>\n",
       "      <td>0.047471</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.010728</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031995</td>\n",
       "      <td>0.029776</td>\n",
       "      <td>0.042443</td>\n",
       "      <td>0.052549</td>\n",
       "      <td>0.079648</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.078980</td>\n",
       "      <td>0.033584</td>\n",
       "      <td>0.038995</td>\n",
       "      <td>0.016729</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.059655</td>\n",
       "      <td>0.017477</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.013011</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.039175</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.022724</td>\n",
       "      <td>0.028748</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.068728</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.096548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.029221</td>\n",
       "      <td>0.050648</td>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.061501</td>\n",
       "      <td>0.066223</td>\n",
       "      <td>0.066947</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>0.056764</td>\n",
       "      <td>0.019356</td>\n",
       "      <td>0.059017</td>\n",
       "      <td>0.039711</td>\n",
       "      <td>0.060479</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>0.036775</td>\n",
       "      <td>0.063569</td>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.037933</td>\n",
       "      <td>0.116213</td>\n",
       "      <td>0.037961</td>\n",
       "      <td>0.095474</td>\n",
       "      <td>0.027391</td>\n",
       "      <td>0.066752</td>\n",
       "      <td>0.047839</td>\n",
       "      <td>0.335034</td>\n",
       "      <td>0.373212</td>\n",
       "      <td>0.031995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337526</td>\n",
       "      <td>0.553300</td>\n",
       "      <td>0.518870</td>\n",
       "      <td>0.559349</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.557573</td>\n",
       "      <td>0.581518</td>\n",
       "      <td>0.565162</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>0.012962</td>\n",
       "      <td>0.042867</td>\n",
       "      <td>0.510165</td>\n",
       "      <td>0.020676</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.083526</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.025020</td>\n",
       "      <td>0.313835</td>\n",
       "      <td>0.031979</td>\n",
       "      <td>0.063495</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>0.038772</td>\n",
       "      <td>0.067596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.021940</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>0.048673</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.037047</td>\n",
       "      <td>0.044840</td>\n",
       "      <td>0.026903</td>\n",
       "      <td>0.045916</td>\n",
       "      <td>0.101684</td>\n",
       "      <td>0.027388</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>0.028513</td>\n",
       "      <td>0.046255</td>\n",
       "      <td>0.045565</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>0.086828</td>\n",
       "      <td>0.027911</td>\n",
       "      <td>0.065026</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.046636</td>\n",
       "      <td>0.035461</td>\n",
       "      <td>0.215256</td>\n",
       "      <td>0.222459</td>\n",
       "      <td>0.029776</td>\n",
       "      <td>0.337526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377012</td>\n",
       "      <td>0.407007</td>\n",
       "      <td>0.506015</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.504233</td>\n",
       "      <td>0.323512</td>\n",
       "      <td>0.406539</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>0.010224</td>\n",
       "      <td>0.463813</td>\n",
       "      <td>0.015803</td>\n",
       "      <td>0.433659</td>\n",
       "      <td>0.059688</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.023578</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.018502</td>\n",
       "      <td>0.158593</td>\n",
       "      <td>0.006575</td>\n",
       "      <td>0.042330</td>\n",
       "      <td>0.050231</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.052799</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>0.052066</td>\n",
       "      <td>0.048127</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>0.043405</td>\n",
       "      <td>0.043643</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.053411</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.052332</td>\n",
       "      <td>0.028975</td>\n",
       "      <td>0.035377</td>\n",
       "      <td>0.058076</td>\n",
       "      <td>0.036064</td>\n",
       "      <td>0.027698</td>\n",
       "      <td>0.107662</td>\n",
       "      <td>0.037214</td>\n",
       "      <td>0.091213</td>\n",
       "      <td>0.026613</td>\n",
       "      <td>0.062341</td>\n",
       "      <td>0.046977</td>\n",
       "      <td>0.448525</td>\n",
       "      <td>0.389279</td>\n",
       "      <td>0.042443</td>\n",
       "      <td>0.553300</td>\n",
       "      <td>0.377012</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607214</td>\n",
       "      <td>0.660284</td>\n",
       "      <td>0.010434</td>\n",
       "      <td>0.657941</td>\n",
       "      <td>0.544411</td>\n",
       "      <td>0.626124</td>\n",
       "      <td>0.058456</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.033171</td>\n",
       "      <td>0.602230</td>\n",
       "      <td>0.038570</td>\n",
       "      <td>0.015953</td>\n",
       "      <td>0.071130</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.005045</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.224192</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.061694</td>\n",
       "      <td>0.065475</td>\n",
       "      <td>0.082593</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.039001</td>\n",
       "      <td>0.064115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.039066</td>\n",
       "      <td>0.018097</td>\n",
       "      <td>0.038927</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.042535</td>\n",
       "      <td>0.046383</td>\n",
       "      <td>0.046280</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>0.040158</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>0.032690</td>\n",
       "      <td>0.035124</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.018236</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.043714</td>\n",
       "      <td>0.041464</td>\n",
       "      <td>0.029224</td>\n",
       "      <td>0.089327</td>\n",
       "      <td>0.026591</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.018971</td>\n",
       "      <td>0.046049</td>\n",
       "      <td>0.033128</td>\n",
       "      <td>0.346018</td>\n",
       "      <td>0.339387</td>\n",
       "      <td>0.052549</td>\n",
       "      <td>0.518870</td>\n",
       "      <td>0.407007</td>\n",
       "      <td>0.607214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737555</td>\n",
       "      <td>0.012213</td>\n",
       "      <td>0.735187</td>\n",
       "      <td>0.523227</td>\n",
       "      <td>0.677790</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.040793</td>\n",
       "      <td>0.699918</td>\n",
       "      <td>0.012922</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.087070</td>\n",
       "      <td>0.075289</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>0.024207</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.233392</td>\n",
       "      <td>0.010718</td>\n",
       "      <td>0.045273</td>\n",
       "      <td>0.047475</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.010897</td>\n",
       "      <td>0.027449</td>\n",
       "      <td>0.045923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.032058</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.061870</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>0.036835</td>\n",
       "      <td>0.040538</td>\n",
       "      <td>0.034276</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>0.036749</td>\n",
       "      <td>0.055837</td>\n",
       "      <td>0.031850</td>\n",
       "      <td>0.018514</td>\n",
       "      <td>0.022921</td>\n",
       "      <td>0.039382</td>\n",
       "      <td>0.042402</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.075169</td>\n",
       "      <td>0.023471</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.016924</td>\n",
       "      <td>0.041561</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>0.366217</td>\n",
       "      <td>0.362120</td>\n",
       "      <td>0.079648</td>\n",
       "      <td>0.559349</td>\n",
       "      <td>0.506015</td>\n",
       "      <td>0.660284</td>\n",
       "      <td>0.737555</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016825</td>\n",
       "      <td>0.996066</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>0.729750</td>\n",
       "      <td>0.032132</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>0.044463</td>\n",
       "      <td>0.848021</td>\n",
       "      <td>0.010166</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.118321</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.015739</td>\n",
       "      <td>0.019079</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.304679</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.041529</td>\n",
       "      <td>0.043484</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>0.027732</td>\n",
       "      <td>0.046796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.041014</td>\n",
       "      <td>0.024903</td>\n",
       "      <td>0.054759</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.031998</td>\n",
       "      <td>0.034164</td>\n",
       "      <td>0.041372</td>\n",
       "      <td>0.039220</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.046344</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.043468</td>\n",
       "      <td>0.040938</td>\n",
       "      <td>0.031627</td>\n",
       "      <td>0.098570</td>\n",
       "      <td>0.027645</td>\n",
       "      <td>0.080787</td>\n",
       "      <td>0.019311</td>\n",
       "      <td>0.048133</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.010434</td>\n",
       "      <td>0.012213</td>\n",
       "      <td>0.016825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017280</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.047860</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.062748</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>0.025089</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.019852</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.005691</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.113105</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.025919</td>\n",
       "      <td>0.006919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.061706</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.026960</td>\n",
       "      <td>0.037315</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>0.034811</td>\n",
       "      <td>0.033601</td>\n",
       "      <td>0.014434</td>\n",
       "      <td>0.036384</td>\n",
       "      <td>0.056589</td>\n",
       "      <td>0.031919</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.023313</td>\n",
       "      <td>0.038832</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>0.043512</td>\n",
       "      <td>0.075039</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.050690</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.041840</td>\n",
       "      <td>0.030398</td>\n",
       "      <td>0.364833</td>\n",
       "      <td>0.361089</td>\n",
       "      <td>0.078980</td>\n",
       "      <td>0.557573</td>\n",
       "      <td>0.504233</td>\n",
       "      <td>0.657941</td>\n",
       "      <td>0.735187</td>\n",
       "      <td>0.996066</td>\n",
       "      <td>0.017280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.554876</td>\n",
       "      <td>0.727119</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>0.043915</td>\n",
       "      <td>0.845359</td>\n",
       "      <td>0.010428</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>0.117510</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.015212</td>\n",
       "      <td>0.019449</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.012065</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.303606</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>0.038626</td>\n",
       "      <td>0.039844</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.044529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.044954</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>0.048335</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>0.049732</td>\n",
       "      <td>0.054315</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>0.035174</td>\n",
       "      <td>0.041847</td>\n",
       "      <td>0.020092</td>\n",
       "      <td>0.046675</td>\n",
       "      <td>0.048154</td>\n",
       "      <td>0.048982</td>\n",
       "      <td>0.024597</td>\n",
       "      <td>0.025376</td>\n",
       "      <td>0.055825</td>\n",
       "      <td>0.055240</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.089516</td>\n",
       "      <td>0.031415</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.023329</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>0.038791</td>\n",
       "      <td>0.314544</td>\n",
       "      <td>0.328805</td>\n",
       "      <td>0.033584</td>\n",
       "      <td>0.581518</td>\n",
       "      <td>0.323512</td>\n",
       "      <td>0.544411</td>\n",
       "      <td>0.523227</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.554876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.518704</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.011150</td>\n",
       "      <td>0.021029</td>\n",
       "      <td>0.513130</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.063374</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.014798</td>\n",
       "      <td>0.016404</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>0.200713</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.048822</td>\n",
       "      <td>0.048947</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.045963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.054673</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>0.048844</td>\n",
       "      <td>0.052819</td>\n",
       "      <td>0.053978</td>\n",
       "      <td>0.033747</td>\n",
       "      <td>0.056270</td>\n",
       "      <td>0.016955</td>\n",
       "      <td>0.050373</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>0.046861</td>\n",
       "      <td>0.008924</td>\n",
       "      <td>0.034381</td>\n",
       "      <td>0.060342</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.024925</td>\n",
       "      <td>0.134514</td>\n",
       "      <td>0.037741</td>\n",
       "      <td>0.105648</td>\n",
       "      <td>0.027201</td>\n",
       "      <td>0.061425</td>\n",
       "      <td>0.046050</td>\n",
       "      <td>0.386132</td>\n",
       "      <td>0.353362</td>\n",
       "      <td>0.038995</td>\n",
       "      <td>0.565162</td>\n",
       "      <td>0.406539</td>\n",
       "      <td>0.626124</td>\n",
       "      <td>0.677790</td>\n",
       "      <td>0.729750</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.727119</td>\n",
       "      <td>0.518704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049792</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.070251</td>\n",
       "      <td>0.674249</td>\n",
       "      <td>0.020078</td>\n",
       "      <td>0.023001</td>\n",
       "      <td>0.081839</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.010422</td>\n",
       "      <td>0.035040</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.245454</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.060379</td>\n",
       "      <td>0.057933</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.045792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.057312</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.067015</td>\n",
       "      <td>0.007761</td>\n",
       "      <td>0.072599</td>\n",
       "      <td>0.057465</td>\n",
       "      <td>0.052035</td>\n",
       "      <td>0.017466</td>\n",
       "      <td>0.033244</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.032310</td>\n",
       "      <td>0.026329</td>\n",
       "      <td>0.042367</td>\n",
       "      <td>0.030316</td>\n",
       "      <td>0.036186</td>\n",
       "      <td>0.060394</td>\n",
       "      <td>0.049606</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.134785</td>\n",
       "      <td>0.041077</td>\n",
       "      <td>0.114988</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>0.069769</td>\n",
       "      <td>0.055013</td>\n",
       "      <td>0.129842</td>\n",
       "      <td>0.159838</td>\n",
       "      <td>0.016729</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.058456</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>0.032132</td>\n",
       "      <td>0.047860</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>0.049792</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.237294</td>\n",
       "      <td>0.017042</td>\n",
       "      <td>0.109142</td>\n",
       "      <td>0.012959</td>\n",
       "      <td>0.324884</td>\n",
       "      <td>0.006227</td>\n",
       "      <td>0.046639</td>\n",
       "      <td>0.127530</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>0.052138</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>0.054578</td>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.022637</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.033204</td>\n",
       "      <td>0.003490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.007960</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.032407</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.130812</td>\n",
       "      <td>0.017918</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>0.012119</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.017950</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.025483</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.017720</td>\n",
       "      <td>0.015373</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.032566</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.016960</td>\n",
       "      <td>0.007913</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>0.021865</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.012962</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>0.011150</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.167478</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.009951</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.010430</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.012909</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.013897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>0.019124</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.042044</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.046978</td>\n",
       "      <td>0.030392</td>\n",
       "      <td>0.040844</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>0.042482</td>\n",
       "      <td>0.018725</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.026516</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>0.035150</td>\n",
       "      <td>0.042276</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.039316</td>\n",
       "      <td>0.028512</td>\n",
       "      <td>0.063244</td>\n",
       "      <td>0.018842</td>\n",
       "      <td>0.038280</td>\n",
       "      <td>0.033614</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.055968</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.042867</td>\n",
       "      <td>0.010224</td>\n",
       "      <td>0.033171</td>\n",
       "      <td>0.040793</td>\n",
       "      <td>0.044463</td>\n",
       "      <td>0.062748</td>\n",
       "      <td>0.043915</td>\n",
       "      <td>0.021029</td>\n",
       "      <td>0.070251</td>\n",
       "      <td>0.237294</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>0.025061</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.214437</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>0.105905</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>0.034492</td>\n",
       "      <td>0.107928</td>\n",
       "      <td>0.038474</td>\n",
       "      <td>0.024846</td>\n",
       "      <td>0.044513</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>0.014032</td>\n",
       "      <td>0.029229</td>\n",
       "      <td>0.049256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.036095</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>0.047066</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.021442</td>\n",
       "      <td>0.029866</td>\n",
       "      <td>0.022121</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.024440</td>\n",
       "      <td>0.036452</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.024623</td>\n",
       "      <td>0.046247</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.060591</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.021537</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.022946</td>\n",
       "      <td>0.325930</td>\n",
       "      <td>0.320140</td>\n",
       "      <td>0.059655</td>\n",
       "      <td>0.510165</td>\n",
       "      <td>0.463813</td>\n",
       "      <td>0.602230</td>\n",
       "      <td>0.699918</td>\n",
       "      <td>0.848021</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>0.845359</td>\n",
       "      <td>0.513130</td>\n",
       "      <td>0.674249</td>\n",
       "      <td>0.017042</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015587</td>\n",
       "      <td>0.016903</td>\n",
       "      <td>0.096036</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>0.027591</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>0.018693</td>\n",
       "      <td>0.268701</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.032509</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.028806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.015420</td>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.047505</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>0.035177</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>0.034654</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>0.018885</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>0.028342</td>\n",
       "      <td>0.036323</td>\n",
       "      <td>0.029480</td>\n",
       "      <td>0.050328</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.058967</td>\n",
       "      <td>0.014251</td>\n",
       "      <td>0.034898</td>\n",
       "      <td>0.025703</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.017477</td>\n",
       "      <td>0.020676</td>\n",
       "      <td>0.015803</td>\n",
       "      <td>0.038570</td>\n",
       "      <td>0.012922</td>\n",
       "      <td>0.010166</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.010428</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>0.020078</td>\n",
       "      <td>0.109142</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.025061</td>\n",
       "      <td>0.015587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013522</td>\n",
       "      <td>0.089370</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>0.015522</td>\n",
       "      <td>0.349431</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>0.034408</td>\n",
       "      <td>0.025911</td>\n",
       "      <td>0.036610</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.026373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.025177</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>0.115041</td>\n",
       "      <td>0.054812</td>\n",
       "      <td>0.049664</td>\n",
       "      <td>0.043626</td>\n",
       "      <td>0.048223</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>0.043150</td>\n",
       "      <td>0.111335</td>\n",
       "      <td>0.038718</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.031883</td>\n",
       "      <td>0.037275</td>\n",
       "      <td>0.040693</td>\n",
       "      <td>0.047379</td>\n",
       "      <td>0.087965</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>0.081266</td>\n",
       "      <td>0.020389</td>\n",
       "      <td>0.049861</td>\n",
       "      <td>0.034146</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.058059</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.433659</td>\n",
       "      <td>0.015953</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.023001</td>\n",
       "      <td>0.012959</td>\n",
       "      <td>0.167478</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.016903</td>\n",
       "      <td>0.013522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.013130</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.038094</td>\n",
       "      <td>0.043653</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.017899</td>\n",
       "      <td>0.034585</td>\n",
       "      <td>0.056511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.024292</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.044325</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.048879</td>\n",
       "      <td>0.030616</td>\n",
       "      <td>0.049079</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>0.034190</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.034560</td>\n",
       "      <td>0.024628</td>\n",
       "      <td>0.012051</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.049018</td>\n",
       "      <td>0.047430</td>\n",
       "      <td>0.052646</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>0.051636</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.050861</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>0.041894</td>\n",
       "      <td>0.039172</td>\n",
       "      <td>0.114237</td>\n",
       "      <td>0.126805</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.083526</td>\n",
       "      <td>0.059688</td>\n",
       "      <td>0.071130</td>\n",
       "      <td>0.087070</td>\n",
       "      <td>0.118321</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>0.117510</td>\n",
       "      <td>0.063374</td>\n",
       "      <td>0.081839</td>\n",
       "      <td>0.324884</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>0.214437</td>\n",
       "      <td>0.096036</td>\n",
       "      <td>0.089370</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.088772</td>\n",
       "      <td>0.014189</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.115548</td>\n",
       "      <td>0.049362</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>0.036529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.022116</td>\n",
       "      <td>0.019739</td>\n",
       "      <td>0.053464</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.034461</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.035159</td>\n",
       "      <td>0.026654</td>\n",
       "      <td>0.036533</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.024445</td>\n",
       "      <td>0.012522</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>0.031556</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.032629</td>\n",
       "      <td>0.065022</td>\n",
       "      <td>0.013478</td>\n",
       "      <td>0.059901</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.034337</td>\n",
       "      <td>0.023522</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.015347</td>\n",
       "      <td>0.013011</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.075289</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.025089</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.006227</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>0.015001</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.033837</td>\n",
       "      <td>0.036241</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.025918</td>\n",
       "      <td>0.040661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.037105</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.050664</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.042336</td>\n",
       "      <td>0.053637</td>\n",
       "      <td>0.050811</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.032065</td>\n",
       "      <td>0.064104</td>\n",
       "      <td>0.088843</td>\n",
       "      <td>0.042278</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.038931</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.059565</td>\n",
       "      <td>0.047838</td>\n",
       "      <td>0.098270</td>\n",
       "      <td>0.043708</td>\n",
       "      <td>0.043310</td>\n",
       "      <td>0.032059</td>\n",
       "      <td>0.053469</td>\n",
       "      <td>0.045922</td>\n",
       "      <td>0.049676</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>0.015739</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.015212</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.010422</td>\n",
       "      <td>0.046639</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.105905</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>0.015522</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>0.088772</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043454</td>\n",
       "      <td>0.012407</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.067569</td>\n",
       "      <td>0.049367</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>0.026979</td>\n",
       "      <td>0.051858</td>\n",
       "      <td>0.095444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.034056</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.056655</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.077986</td>\n",
       "      <td>0.033046</td>\n",
       "      <td>0.056166</td>\n",
       "      <td>0.037916</td>\n",
       "      <td>0.056817</td>\n",
       "      <td>0.030326</td>\n",
       "      <td>0.050896</td>\n",
       "      <td>0.071947</td>\n",
       "      <td>0.021946</td>\n",
       "      <td>0.020994</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.047441</td>\n",
       "      <td>0.057915</td>\n",
       "      <td>0.039857</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.030897</td>\n",
       "      <td>0.081662</td>\n",
       "      <td>0.020849</td>\n",
       "      <td>0.053271</td>\n",
       "      <td>0.031401</td>\n",
       "      <td>0.045916</td>\n",
       "      <td>0.037212</td>\n",
       "      <td>0.039175</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.023578</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.024207</td>\n",
       "      <td>0.019079</td>\n",
       "      <td>0.019852</td>\n",
       "      <td>0.019449</td>\n",
       "      <td>0.014798</td>\n",
       "      <td>0.035040</td>\n",
       "      <td>0.127530</td>\n",
       "      <td>0.009951</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.027591</td>\n",
       "      <td>0.349431</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.014189</td>\n",
       "      <td>0.015001</td>\n",
       "      <td>0.043454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010370</td>\n",
       "      <td>0.015719</td>\n",
       "      <td>0.015382</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.028845</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.033365</td>\n",
       "      <td>0.046371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.009818</td>\n",
       "      <td>0.029339</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.014343</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.015546</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.049647</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.017837</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.021317</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.017122</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.033909</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.005045</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.016404</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.013130</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.012407</td>\n",
       "      <td>0.010370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>0.010154</td>\n",
       "      <td>0.005158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.026344</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>0.031408</td>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.026017</td>\n",
       "      <td>0.016842</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.019746</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>0.028084</td>\n",
       "      <td>0.029947</td>\n",
       "      <td>0.017173</td>\n",
       "      <td>0.040015</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.050951</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>0.030155</td>\n",
       "      <td>0.019171</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.012065</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.015719</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.026576</td>\n",
       "      <td>0.030751</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.016894</td>\n",
       "      <td>0.010033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>0.033213</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.032759</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.033089</td>\n",
       "      <td>0.027432</td>\n",
       "      <td>0.014646</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>0.023445</td>\n",
       "      <td>0.019045</td>\n",
       "      <td>0.018277</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>0.031542</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>0.044314</td>\n",
       "      <td>0.020851</td>\n",
       "      <td>0.058660</td>\n",
       "      <td>0.416608</td>\n",
       "      <td>0.027362</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>0.029181</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>0.022724</td>\n",
       "      <td>0.025020</td>\n",
       "      <td>0.018502</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.005691</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.052138</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.034492</td>\n",
       "      <td>0.018693</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>0.015382</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049124</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.055057</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.055298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.049837</td>\n",
       "      <td>0.016495</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.051885</td>\n",
       "      <td>0.032494</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>0.051151</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.046578</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.128882</td>\n",
       "      <td>0.021431</td>\n",
       "      <td>0.085181</td>\n",
       "      <td>0.046244</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.136979</td>\n",
       "      <td>0.144771</td>\n",
       "      <td>0.028748</td>\n",
       "      <td>0.313835</td>\n",
       "      <td>0.158593</td>\n",
       "      <td>0.224192</td>\n",
       "      <td>0.233392</td>\n",
       "      <td>0.304679</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.303606</td>\n",
       "      <td>0.200713</td>\n",
       "      <td>0.245454</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.010430</td>\n",
       "      <td>0.107928</td>\n",
       "      <td>0.268701</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>0.049124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022316</td>\n",
       "      <td>0.030354</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.034365</td>\n",
       "      <td>0.370963</td>\n",
       "      <td>0.112209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.015133</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.019548</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.025183</td>\n",
       "      <td>0.044966</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>0.014349</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.063826</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>0.045469</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.020798</td>\n",
       "      <td>0.039723</td>\n",
       "      <td>0.064349</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.031979</td>\n",
       "      <td>0.006575</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.010718</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.113105</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.038474</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.034408</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.115548</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.022316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.008180</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>0.006016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.058292</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.065043</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.043639</td>\n",
       "      <td>0.036737</td>\n",
       "      <td>0.024992</td>\n",
       "      <td>0.013369</td>\n",
       "      <td>0.040737</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.018607</td>\n",
       "      <td>0.104261</td>\n",
       "      <td>0.077049</td>\n",
       "      <td>0.039350</td>\n",
       "      <td>0.153381</td>\n",
       "      <td>0.048350</td>\n",
       "      <td>0.084017</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.070103</td>\n",
       "      <td>0.051076</td>\n",
       "      <td>0.090862</td>\n",
       "      <td>0.078367</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.063495</td>\n",
       "      <td>0.042330</td>\n",
       "      <td>0.061694</td>\n",
       "      <td>0.045273</td>\n",
       "      <td>0.041529</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>0.038626</td>\n",
       "      <td>0.048822</td>\n",
       "      <td>0.060379</td>\n",
       "      <td>0.054578</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.024846</td>\n",
       "      <td>0.032509</td>\n",
       "      <td>0.025911</td>\n",
       "      <td>0.038094</td>\n",
       "      <td>0.049362</td>\n",
       "      <td>0.033837</td>\n",
       "      <td>0.067569</td>\n",
       "      <td>0.028845</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.026576</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.030354</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.054308</td>\n",
       "      <td>0.077392</td>\n",
       "      <td>0.036321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.117419</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.087618</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.070127</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.149365</td>\n",
       "      <td>0.075786</td>\n",
       "      <td>0.070227</td>\n",
       "      <td>0.016723</td>\n",
       "      <td>0.205905</td>\n",
       "      <td>0.080953</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>0.049953</td>\n",
       "      <td>0.098323</td>\n",
       "      <td>0.063872</td>\n",
       "      <td>0.091470</td>\n",
       "      <td>0.034948</td>\n",
       "      <td>0.141649</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.104691</td>\n",
       "      <td>0.086634</td>\n",
       "      <td>0.081198</td>\n",
       "      <td>0.068728</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.050231</td>\n",
       "      <td>0.065475</td>\n",
       "      <td>0.047475</td>\n",
       "      <td>0.043484</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.039844</td>\n",
       "      <td>0.048947</td>\n",
       "      <td>0.057933</td>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.012909</td>\n",
       "      <td>0.044513</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.036610</td>\n",
       "      <td>0.043653</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>0.036241</td>\n",
       "      <td>0.049367</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>0.030751</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>0.183144</td>\n",
       "      <td>0.201948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.046612</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.030445</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.035534</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.020978</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.007214</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.184428</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>0.020691</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.082593</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.022637</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.055057</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013497</td>\n",
       "      <td>0.061657</td>\n",
       "      <td>0.042568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.044491</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.097398</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.041565</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.111308</td>\n",
       "      <td>0.073677</td>\n",
       "      <td>0.029258</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.015036</td>\n",
       "      <td>0.038126</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.030592</td>\n",
       "      <td>0.067140</td>\n",
       "      <td>0.041066</td>\n",
       "      <td>0.021497</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.024234</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.010897</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.014032</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.017899</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.026979</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.034365</td>\n",
       "      <td>0.008180</td>\n",
       "      <td>0.054308</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>0.013497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.492638</td>\n",
       "      <td>0.162314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.061382</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.107463</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>0.090172</td>\n",
       "      <td>0.059677</td>\n",
       "      <td>0.037575</td>\n",
       "      <td>0.189247</td>\n",
       "      <td>0.103308</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.060993</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.062672</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>0.099463</td>\n",
       "      <td>0.085321</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.123036</td>\n",
       "      <td>0.044870</td>\n",
       "      <td>0.051206</td>\n",
       "      <td>0.051806</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.038772</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.039001</td>\n",
       "      <td>0.027449</td>\n",
       "      <td>0.027732</td>\n",
       "      <td>0.025919</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.033204</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.029229</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.034585</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>0.025918</td>\n",
       "      <td>0.051858</td>\n",
       "      <td>0.033365</td>\n",
       "      <td>0.010154</td>\n",
       "      <td>0.016894</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.370963</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>0.077392</td>\n",
       "      <td>0.183144</td>\n",
       "      <td>0.061657</td>\n",
       "      <td>0.492638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.089165</td>\n",
       "      <td>0.022680</td>\n",
       "      <td>0.070114</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>0.008344</td>\n",
       "      <td>0.040252</td>\n",
       "      <td>0.248724</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.115055</td>\n",
       "      <td>0.020076</td>\n",
       "      <td>0.105150</td>\n",
       "      <td>0.169257</td>\n",
       "      <td>0.151626</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.064261</td>\n",
       "      <td>0.046364</td>\n",
       "      <td>0.007307</td>\n",
       "      <td>0.075751</td>\n",
       "      <td>0.051797</td>\n",
       "      <td>0.103954</td>\n",
       "      <td>0.165977</td>\n",
       "      <td>0.080993</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>0.059601</td>\n",
       "      <td>0.096548</td>\n",
       "      <td>0.067596</td>\n",
       "      <td>0.056628</td>\n",
       "      <td>0.064115</td>\n",
       "      <td>0.045923</td>\n",
       "      <td>0.046796</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>0.045963</td>\n",
       "      <td>0.045792</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.049256</td>\n",
       "      <td>0.028806</td>\n",
       "      <td>0.026373</td>\n",
       "      <td>0.056511</td>\n",
       "      <td>0.036529</td>\n",
       "      <td>0.040661</td>\n",
       "      <td>0.095444</td>\n",
       "      <td>0.046371</td>\n",
       "      <td>0.005158</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>0.112209</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.036321</td>\n",
       "      <td>0.201948</td>\n",
       "      <td>0.042568</td>\n",
       "      <td>0.162314</td>\n",
       "      <td>0.475486</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   1.000000  0.016759  0.065627  0.013273  0.023119  0.059674  0.007669   \n",
       "1   0.016759  1.000000  0.033526  0.006923  0.023760  0.024840  0.003918   \n",
       "2   0.065627  0.033526  1.000000  0.020246  0.077734  0.087564  0.036677   \n",
       "3   0.013273  0.006923  0.020246  1.000000  0.003238  0.010014  0.019784   \n",
       "4   0.023119  0.023760  0.077734  0.003238  1.000000  0.054054  0.147336   \n",
       "5   0.059674  0.024840  0.087564  0.010014  0.054054  1.000000  0.061163   \n",
       "6   0.007669  0.003918  0.036677  0.019784  0.147336  0.061163  1.000000   \n",
       "7   0.003950  0.016280  0.012003  0.010268  0.029598  0.079561  0.044545   \n",
       "8   0.106263  0.003826  0.093786  0.002454  0.020823  0.117438  0.050786   \n",
       "9   0.041198  0.032962  0.032075  0.004947  0.034495  0.013897  0.056809   \n",
       "10  0.188459  0.006864  0.048254  0.012976  0.068382  0.053900  0.159578   \n",
       "11  0.105801  0.040398  0.083210  0.019221  0.066788  0.009264  0.001461   \n",
       "12  0.066438  0.018858  0.047593  0.013199  0.031126  0.077631  0.013295   \n",
       "13  0.036780  0.009206  0.008552  0.012008  0.003445  0.009673  0.022723   \n",
       "14  0.028439  0.005330  0.122113  0.002707  0.056177  0.173066  0.042904   \n",
       "15  0.059386  0.009117  0.063906  0.007432  0.083024  0.019865  0.128436   \n",
       "16  0.081928  0.018370  0.036262  0.003470  0.143443  0.064137  0.187981   \n",
       "17  0.053324  0.033500  0.121923  0.019391  0.062344  0.078350  0.122011   \n",
       "18  0.128243  0.055476  0.139329  0.010834  0.098510  0.095505  0.111792   \n",
       "19  0.021295  0.015806  0.031111  0.005381  0.031526  0.058979  0.046134   \n",
       "20  0.197049  0.018191  0.156651  0.008176  0.136605  0.106833  0.130794   \n",
       "21  0.024349  0.008850  0.035681  0.028102  0.020207  0.007956  0.002093   \n",
       "22  0.134072  0.020502  0.123671  0.011368  0.070037  0.211455  0.064795   \n",
       "23  0.188155  0.001984  0.041145  0.035360  0.000039  0.059329  0.030575   \n",
       "24  0.072504  0.043483  0.087924  0.015181  0.072502  0.084402  0.089494   \n",
       "25  0.061686  0.038211  0.062459  0.013708  0.075456  0.087271  0.080330   \n",
       "26  0.066424  0.030307  0.108886  0.010684  0.088011  0.069051  0.065893   \n",
       "27  0.048680  0.029221  0.050648  0.010368  0.061501  0.066223  0.066947   \n",
       "28  0.041251  0.021940  0.057726  0.007798  0.032048  0.048673  0.048482   \n",
       "29  0.052799  0.027508  0.032547  0.010476  0.052066  0.048127  0.058101   \n",
       "30  0.039066  0.018097  0.038927  0.007529  0.042535  0.046383  0.046280   \n",
       "31  0.032058  0.003326  0.061870  0.006717  0.026748  0.036835  0.040538   \n",
       "32  0.041014  0.024903  0.054759  0.008075  0.031998  0.034164  0.041372   \n",
       "33  0.027690  0.004303  0.061706  0.006729  0.026960  0.037315  0.040910   \n",
       "34  0.044954  0.024058  0.048335  0.006122  0.049732  0.054315  0.053202   \n",
       "35  0.054673  0.028198  0.046504  0.006515  0.048844  0.052819  0.053978   \n",
       "36  0.057312  0.024013  0.067015  0.007761  0.072599  0.057465  0.052035   \n",
       "37  0.007960  0.008922  0.032407  0.002669  0.130812  0.017918  0.014781   \n",
       "38  0.011134  0.019124  0.014809  0.004602  0.042044  0.047619  0.046978   \n",
       "39  0.036095  0.014821  0.047066  0.007643  0.021442  0.029866  0.022121   \n",
       "40  0.009703  0.015420  0.030956  0.005670  0.047505  0.029457  0.033120   \n",
       "41  0.026070  0.025177  0.005811  0.008095  0.115041  0.054812  0.049664   \n",
       "42  0.024292  0.002370  0.044325  0.009268  0.048879  0.030616  0.049079   \n",
       "43  0.022116  0.019739  0.053464  0.005933  0.015234  0.028826  0.034461   \n",
       "44  0.037105  0.016418  0.050664  0.012957  0.042336  0.053637  0.050811   \n",
       "45  0.034056  0.023858  0.056655  0.009181  0.077986  0.033046  0.056166   \n",
       "46  0.000953  0.009818  0.029339  0.003348  0.026900  0.014343  0.017512   \n",
       "47  0.017755  0.015747  0.026344  0.001924  0.032005  0.031693  0.031408   \n",
       "48  0.026505  0.007282  0.033213  0.000591  0.032759  0.019119  0.033089   \n",
       "49  0.021196  0.049837  0.016495  0.012370  0.046361  0.008705  0.051885   \n",
       "50  0.033301  0.018527  0.033120  0.007148  0.026390  0.015133  0.027653   \n",
       "51  0.058292  0.014461  0.108140  0.003138  0.025509  0.065043  0.053706   \n",
       "52  0.117419  0.009605  0.087618  0.010862  0.041582  0.105692  0.070127   \n",
       "53  0.008844  0.001946  0.003336  0.000298  0.002016  0.019894  0.046612   \n",
       "54  0.044491  0.002083  0.097398  0.005260  0.052662  0.010278  0.041565   \n",
       "55  0.061382  0.000271  0.107463  0.022081  0.052290  0.090172  0.059677   \n",
       "56  0.089165  0.022680  0.070114  0.021369  0.002492  0.082089  0.008344   \n",
       "\n",
       "           7         8         9        10        11        12        13  \\\n",
       "0   0.003950  0.106263  0.041198  0.188459  0.105801  0.066438  0.036780   \n",
       "1   0.016280  0.003826  0.032962  0.006864  0.040398  0.018858  0.009206   \n",
       "2   0.012003  0.093786  0.032075  0.048254  0.083210  0.047593  0.008552   \n",
       "3   0.010268  0.002454  0.004947  0.012976  0.019221  0.013199  0.012008   \n",
       "4   0.029598  0.020823  0.034495  0.068382  0.066788  0.031126  0.003445   \n",
       "5   0.079561  0.117438  0.013897  0.053900  0.009264  0.077631  0.009673   \n",
       "6   0.044545  0.050786  0.056809  0.159578  0.001461  0.013295  0.022723   \n",
       "7   1.000000  0.105302  0.083129  0.128495  0.002973  0.026274  0.012426   \n",
       "8   0.105302  1.000000  0.130624  0.137760  0.030344  0.034738  0.066840   \n",
       "9   0.083129  0.130624  1.000000  0.125319  0.071157  0.045737  0.017901   \n",
       "10  0.128495  0.137760  0.125319  1.000000  0.118259  0.049203  0.046687   \n",
       "11  0.002973  0.030344  0.071157  0.118259  1.000000  0.003998  0.006614   \n",
       "12  0.026274  0.034738  0.045737  0.049203  0.003998  1.000000  0.063683   \n",
       "13  0.012426  0.066840  0.017901  0.046687  0.006614  0.063683  1.000000   \n",
       "14  0.072782  0.238436  0.160543  0.064718  0.023152  0.079730  0.018002   \n",
       "15  0.051115  0.008269  0.025601  0.098711  0.028699  0.004402  0.002947   \n",
       "16  0.216422  0.158390  0.081363  0.174198  0.055409  0.049218  0.013996   \n",
       "17  0.037738  0.098804  0.035977  0.089641  0.012901  0.070942  0.028065   \n",
       "18  0.020641  0.039017  0.093509  0.141780  0.088045  0.118721  0.018087   \n",
       "19  0.109163  0.123217  0.030859  0.155769  0.012239  0.016798  0.032940   \n",
       "20  0.156905  0.159112  0.098072  0.294826  0.105631  0.049089  0.043854   \n",
       "21  0.016192  0.019648  0.008200  0.010489  0.045031  0.029978  0.019458   \n",
       "22  0.089226  0.126800  0.096809  0.106324  0.016007  0.119115  0.044354   \n",
       "23  0.034127  0.099461  0.052129  0.056069  0.019324  0.078374  0.031922   \n",
       "24  0.053038  0.069931  0.033534  0.075640  0.021755  0.059361  0.035957   \n",
       "25  0.041450  0.049775  0.013045  0.076468  0.010483  0.067375  0.042133   \n",
       "26  0.057189  0.064608  0.067817  0.065010  0.119059  0.054467  0.028039   \n",
       "27  0.049988  0.056764  0.019356  0.059017  0.039711  0.060479  0.019754   \n",
       "28  0.037047  0.044840  0.026903  0.045916  0.101684  0.027388  0.011633   \n",
       "29  0.043405  0.043643  0.008677  0.053411  0.011963  0.052332  0.028975   \n",
       "30  0.035816  0.040158  0.024423  0.032690  0.035124  0.036798  0.018236   \n",
       "31  0.034276  0.033984  0.015137  0.036749  0.055837  0.031850  0.018514   \n",
       "32  0.039220  0.014403  0.035366  0.041661  0.019918  0.046344  0.011694   \n",
       "33  0.034811  0.033601  0.014434  0.036384  0.056589  0.031919  0.018724   \n",
       "34  0.035174  0.041847  0.020092  0.046675  0.048154  0.048982  0.024597   \n",
       "35  0.033747  0.056270  0.016955  0.050373  0.010626  0.046861  0.008924   \n",
       "36  0.017466  0.033244  0.004944  0.032310  0.026329  0.042367  0.030316   \n",
       "37  0.012119  0.002216  0.017950  0.004917  0.025483  0.014977  0.002072   \n",
       "38  0.030392  0.040844  0.016091  0.042482  0.018725  0.016401  0.026516   \n",
       "39  0.005988  0.009867  0.004163  0.024440  0.036452  0.013411  0.024623   \n",
       "40  0.003884  0.035177  0.025084  0.034654  0.023197  0.018885  0.020966   \n",
       "41  0.043626  0.048223  0.054467  0.043150  0.111335  0.038718  0.005206   \n",
       "42  0.004542  0.034190  0.023200  0.034560  0.024628  0.012051  0.004633   \n",
       "43  0.030134  0.035159  0.026654  0.036533  0.016627  0.024445  0.012522   \n",
       "44  0.002423  0.075558  0.032065  0.064104  0.088843  0.042278  0.000577   \n",
       "45  0.037916  0.056817  0.030326  0.050896  0.071947  0.021946  0.020994   \n",
       "46  0.006397  0.007521  0.015546  0.019159  0.001015  0.012778  0.049647   \n",
       "47  0.021224  0.026017  0.016842  0.022689  0.032943  0.019746  0.016425   \n",
       "48  0.027432  0.014646  0.011945  0.032410  0.027711  0.023445  0.019045   \n",
       "49  0.032494  0.031003  0.003936  0.055089  0.030940  0.051151  0.005804   \n",
       "50  0.019548  0.013601  0.007357  0.025183  0.044966  0.028283  0.014349   \n",
       "51  0.031454  0.043639  0.036737  0.024992  0.013369  0.040737  0.008499   \n",
       "52  0.057910  0.149365  0.075786  0.070227  0.016723  0.205905  0.080953   \n",
       "53  0.008012  0.000522  0.044830  0.001126  0.030445  0.014195  0.006545   \n",
       "54  0.011254  0.111308  0.073677  0.029258  0.010002  0.013446  0.003023   \n",
       "55  0.037575  0.189247  0.103308  0.086791  0.021774  0.041962  0.060993   \n",
       "56  0.040252  0.248724  0.087273  0.115055  0.020076  0.105150  0.169257   \n",
       "\n",
       "          14        15        16        17        18        19        20  \\\n",
       "0   0.028439  0.059386  0.081928  0.053324  0.128243  0.021295  0.197049   \n",
       "1   0.005330  0.009117  0.018370  0.033500  0.055476  0.015806  0.018191   \n",
       "2   0.122113  0.063906  0.036262  0.121923  0.139329  0.031111  0.156651   \n",
       "3   0.002707  0.007432  0.003470  0.019391  0.010834  0.005381  0.008176   \n",
       "4   0.056177  0.083024  0.143443  0.062344  0.098510  0.031526  0.136605   \n",
       "5   0.173066  0.019865  0.064137  0.078350  0.095505  0.058979  0.106833   \n",
       "6   0.042904  0.128436  0.187981  0.122011  0.111792  0.046134  0.130794   \n",
       "7   0.072782  0.051115  0.216422  0.037738  0.020641  0.109163  0.156905   \n",
       "8   0.238436  0.008269  0.158390  0.098804  0.039017  0.123217  0.159112   \n",
       "9   0.160543  0.025601  0.081363  0.035977  0.093509  0.030859  0.098072   \n",
       "10  0.064718  0.098711  0.174198  0.089641  0.141780  0.155769  0.294826   \n",
       "11  0.023152  0.028699  0.055409  0.012901  0.088045  0.012239  0.105631   \n",
       "12  0.079730  0.004402  0.049218  0.070942  0.118721  0.016798  0.049089   \n",
       "13  0.018002  0.002947  0.013996  0.028065  0.018087  0.032940  0.043854   \n",
       "14  1.000000  0.000612  0.019339  0.254489  0.040029  0.016516  0.073393   \n",
       "15  0.000612  1.000000  0.055753  0.072007  0.081897  0.027665  0.102815   \n",
       "16  0.019339  0.055753  1.000000  0.049630  0.084983  0.190948  0.221831   \n",
       "17  0.254489  0.072007  0.049630  1.000000  0.094580  0.012939  0.137954   \n",
       "18  0.040029  0.081897  0.084983  0.094580  1.000000  0.037842  0.304986   \n",
       "19  0.016516  0.027665  0.190948  0.012939  0.037842  1.000000  0.133062   \n",
       "20  0.073393  0.102815  0.221831  0.137954  0.304986  0.133062  1.000000   \n",
       "21  0.004291  0.006885  0.021045  0.026496  0.023681  0.033884  0.021361   \n",
       "22  0.369588  0.055398  0.097284  0.092498  0.115905  0.041077  0.118533   \n",
       "23  0.034731  0.096812  0.044865  0.092390  0.184075  0.060560  0.165361   \n",
       "24  0.048988  0.090139  0.052977  0.038272  0.197683  0.048038  0.154633   \n",
       "25  0.031237  0.079237  0.076310  0.028572  0.162800  0.049375  0.135311   \n",
       "26  0.041069  0.001308  0.069724  0.067658  0.154758  0.037469  0.126247   \n",
       "27  0.036775  0.063569  0.063895  0.037933  0.116213  0.037961  0.095474   \n",
       "28  0.028513  0.046255  0.045565  0.051942  0.086828  0.027911  0.065026   \n",
       "29  0.035377  0.058076  0.036064  0.027698  0.107662  0.037214  0.091213   \n",
       "30  0.026466  0.043714  0.041464  0.029224  0.089327  0.026591  0.068571   \n",
       "31  0.022921  0.039382  0.042402  0.042481  0.075169  0.023471  0.050304   \n",
       "32  0.015010  0.043468  0.040938  0.031627  0.098570  0.027645  0.080787   \n",
       "33  0.023313  0.038832  0.042048  0.043512  0.075039  0.024194  0.050690   \n",
       "34  0.025376  0.055825  0.055240  0.021571  0.089516  0.031415  0.080500   \n",
       "35  0.034381  0.060342  0.031863  0.024925  0.134514  0.037741  0.105648   \n",
       "36  0.036186  0.060394  0.049606  0.039107  0.134785  0.041077  0.114988   \n",
       "37  0.011316  0.017720  0.015373  0.006375  0.032566  0.009671  0.007160   \n",
       "38  0.016444  0.035150  0.042276  0.027579  0.039316  0.028512  0.063244   \n",
       "39  0.046247  0.029906  0.009064  0.005743  0.060591  0.002970  0.016888   \n",
       "40  0.013241  0.028342  0.036323  0.029480  0.050328  0.020116  0.058967   \n",
       "41  0.031883  0.037275  0.040693  0.047379  0.087965  0.028773  0.081266   \n",
       "42  0.049018  0.047430  0.052646  0.019975  0.051636  0.028329  0.050861   \n",
       "43  0.017571  0.031556  0.021097  0.032629  0.065022  0.013478  0.059901   \n",
       "44  0.038931  0.045653  0.059565  0.047838  0.098270  0.043708  0.043310   \n",
       "45  0.024221  0.047441  0.057915  0.039857  0.009787  0.030897  0.081662   \n",
       "46  0.011387  0.017837  0.007799  0.021317  0.001212  0.007537  0.012737   \n",
       "47  0.020818  0.028084  0.029947  0.017173  0.040015  0.018198  0.050951   \n",
       "48  0.018277  0.026841  0.031542  0.039519  0.044314  0.020851  0.058660   \n",
       "49  0.002551  0.046578  0.035897  0.035897  0.128882  0.021431  0.085181   \n",
       "50  0.003111  0.029560  0.036691  0.017439  0.063826  0.012071  0.045469   \n",
       "51  0.018607  0.104261  0.077049  0.039350  0.153381  0.048350  0.084017   \n",
       "52  0.123854  0.049953  0.098323  0.063872  0.091470  0.034948  0.141649   \n",
       "53  0.005446  0.035534  0.000466  0.020978  0.002434  0.007214  0.004355   \n",
       "54  0.017383  0.015036  0.038126  0.007979  0.030592  0.067140  0.041066   \n",
       "55  0.213992  0.026528  0.062672  0.075122  0.006530  0.099463  0.085321   \n",
       "56  0.151626  0.003007  0.064261  0.046364  0.007307  0.075751  0.051797   \n",
       "\n",
       "          21        22        23        24        25        26        27  \\\n",
       "0   0.024349  0.134072  0.188155  0.072504  0.061686  0.066424  0.048680   \n",
       "1   0.008850  0.020502  0.001984  0.043483  0.038211  0.030307  0.029221   \n",
       "2   0.035681  0.123671  0.041145  0.087924  0.062459  0.108886  0.050648   \n",
       "3   0.028102  0.011368  0.035360  0.015181  0.013708  0.010684  0.010368   \n",
       "4   0.020207  0.070037  0.000039  0.072502  0.075456  0.088011  0.061501   \n",
       "5   0.007956  0.211455  0.059329  0.084402  0.087271  0.069051  0.066223   \n",
       "6   0.002093  0.064795  0.030575  0.089494  0.080330  0.065893  0.066947   \n",
       "7   0.016192  0.089226  0.034127  0.053038  0.041450  0.057189  0.049988   \n",
       "8   0.019648  0.126800  0.099461  0.069931  0.049775  0.064608  0.056764   \n",
       "9   0.008200  0.096809  0.052129  0.033534  0.013045  0.067817  0.019356   \n",
       "10  0.010489  0.106324  0.056069  0.075640  0.076468  0.065010  0.059017   \n",
       "11  0.045031  0.016007  0.019324  0.021755  0.010483  0.119059  0.039711   \n",
       "12  0.029978  0.119115  0.078374  0.059361  0.067375  0.054467  0.060479   \n",
       "13  0.019458  0.044354  0.031922  0.035957  0.042133  0.028039  0.019754   \n",
       "14  0.004291  0.369588  0.034731  0.048988  0.031237  0.041069  0.036775   \n",
       "15  0.006885  0.055398  0.096812  0.090139  0.079237  0.001308  0.063569   \n",
       "16  0.021045  0.097284  0.044865  0.052977  0.076310  0.069724  0.063895   \n",
       "17  0.026496  0.092498  0.092390  0.038272  0.028572  0.067658  0.037933   \n",
       "18  0.023681  0.115905  0.184075  0.197683  0.162800  0.154758  0.116213   \n",
       "19  0.033884  0.041077  0.060560  0.048038  0.049375  0.037469  0.037961   \n",
       "20  0.021361  0.118533  0.165361  0.154633  0.135311  0.126247  0.095474   \n",
       "21  1.000000  0.023165  0.010915  0.038675  0.035020  0.026931  0.027391   \n",
       "22  0.023165  1.000000  0.052587  0.086034  0.081171  0.066007  0.066752   \n",
       "23  0.010915  0.052587  1.000000  0.067306  0.061840  0.047471  0.047839   \n",
       "24  0.038675  0.086034  0.067306  1.000000  0.517293  0.008861  0.335034   \n",
       "25  0.035020  0.081171  0.061840  0.517293  1.000000  0.010728  0.373212   \n",
       "26  0.026931  0.066007  0.047471  0.008861  0.010728  1.000000  0.031995   \n",
       "27  0.027391  0.066752  0.047839  0.335034  0.373212  0.031995  1.000000   \n",
       "28  0.019702  0.046636  0.035461  0.215256  0.222459  0.029776  0.337526   \n",
       "29  0.026613  0.062341  0.046977  0.448525  0.389279  0.042443  0.553300   \n",
       "30  0.018971  0.046049  0.033128  0.346018  0.339387  0.052549  0.518870   \n",
       "31  0.016924  0.041561  0.029727  0.366217  0.362120  0.079648  0.559349   \n",
       "32  0.019311  0.048133  0.035366  0.004174  0.000385  0.020900  0.001584   \n",
       "33  0.017151  0.041840  0.030398  0.364833  0.361089  0.078980  0.557573   \n",
       "34  0.023329  0.049853  0.038791  0.314544  0.328805  0.033584  0.581518   \n",
       "35  0.027201  0.061425  0.046050  0.386132  0.353362  0.038995  0.565162   \n",
       "36  0.033055  0.069769  0.055013  0.129842  0.159838  0.016729  0.022082   \n",
       "37  0.004947  0.016960  0.007913  0.008041  0.021865  0.009969  0.012962   \n",
       "38  0.018842  0.038280  0.033614  0.050343  0.055968  0.003113  0.042867   \n",
       "39  0.021537  0.003530  0.022946  0.325930  0.320140  0.059655  0.510165   \n",
       "40  0.014251  0.034898  0.025703  0.001757  0.016286  0.017477  0.020676   \n",
       "41  0.020389  0.049861  0.034146  0.019469  0.058059  0.005493  0.003988   \n",
       "42  0.021567  0.041894  0.039172  0.114237  0.126805  0.010786  0.083526   \n",
       "43  0.011931  0.034337  0.023522  0.001717  0.015347  0.013011  0.000017   \n",
       "44  0.032059  0.053469  0.045922  0.049676  0.007137  0.006860  0.003867   \n",
       "45  0.020849  0.053271  0.031401  0.045916  0.037212  0.039175  0.008822   \n",
       "46  0.008436  0.017122  0.010458  0.002627  0.033909  0.010085  0.003955   \n",
       "47  0.013168  0.030155  0.019171  0.002739  0.032860  0.007194  0.004689   \n",
       "48  0.416608  0.027362  0.019139  0.029181  0.013558  0.022724  0.025020   \n",
       "49  0.046244  0.033174  0.033113  0.136979  0.144771  0.028748  0.313835   \n",
       "50  0.001137  0.000467  0.020798  0.039723  0.064349  0.017676  0.031979   \n",
       "51  0.004838  0.070103  0.051076  0.090862  0.078367  0.067500  0.063495   \n",
       "52  0.011036  0.310971  0.104691  0.086634  0.081198  0.068728  0.061441   \n",
       "53  0.184428  0.020140  0.000703  0.058780  0.020691  0.020561  0.011438   \n",
       "54  0.021497  0.008372  0.007681  0.017285  0.024234  0.025504  0.013757   \n",
       "55  0.027775  0.123036  0.044870  0.051206  0.051806  0.054400  0.038772   \n",
       "56  0.103954  0.165977  0.080993  0.043267  0.059601  0.096548  0.067596   \n",
       "\n",
       "          28        29        30        31        32        33        34  \\\n",
       "0   0.041251  0.052799  0.039066  0.032058  0.041014  0.027690  0.044954   \n",
       "1   0.021940  0.027508  0.018097  0.003326  0.024903  0.004303  0.024058   \n",
       "2   0.057726  0.032547  0.038927  0.061870  0.054759  0.061706  0.048335   \n",
       "3   0.007798  0.010476  0.007529  0.006717  0.008075  0.006729  0.006122   \n",
       "4   0.032048  0.052066  0.042535  0.026748  0.031998  0.026960  0.049732   \n",
       "5   0.048673  0.048127  0.046383  0.036835  0.034164  0.037315  0.054315   \n",
       "6   0.048482  0.058101  0.046280  0.040538  0.041372  0.040910  0.053202   \n",
       "7   0.037047  0.043405  0.035816  0.034276  0.039220  0.034811  0.035174   \n",
       "8   0.044840  0.043643  0.040158  0.033984  0.014403  0.033601  0.041847   \n",
       "9   0.026903  0.008677  0.024423  0.015137  0.035366  0.014434  0.020092   \n",
       "10  0.045916  0.053411  0.032690  0.036749  0.041661  0.036384  0.046675   \n",
       "11  0.101684  0.011963  0.035124  0.055837  0.019918  0.056589  0.048154   \n",
       "12  0.027388  0.052332  0.036798  0.031850  0.046344  0.031919  0.048982   \n",
       "13  0.011633  0.028975  0.018236  0.018514  0.011694  0.018724  0.024597   \n",
       "14  0.028513  0.035377  0.026466  0.022921  0.015010  0.023313  0.025376   \n",
       "15  0.046255  0.058076  0.043714  0.039382  0.043468  0.038832  0.055825   \n",
       "16  0.045565  0.036064  0.041464  0.042402  0.040938  0.042048  0.055240   \n",
       "17  0.051942  0.027698  0.029224  0.042481  0.031627  0.043512  0.021571   \n",
       "18  0.086828  0.107662  0.089327  0.075169  0.098570  0.075039  0.089516   \n",
       "19  0.027911  0.037214  0.026591  0.023471  0.027645  0.024194  0.031415   \n",
       "20  0.065026  0.091213  0.068571  0.050304  0.080787  0.050690  0.080500   \n",
       "21  0.019702  0.026613  0.018971  0.016924  0.019311  0.017151  0.023329   \n",
       "22  0.046636  0.062341  0.046049  0.041561  0.048133  0.041840  0.049853   \n",
       "23  0.035461  0.046977  0.033128  0.029727  0.035366  0.030398  0.038791   \n",
       "24  0.215256  0.448525  0.346018  0.366217  0.004174  0.364833  0.314544   \n",
       "25  0.222459  0.389279  0.339387  0.362120  0.000385  0.361089  0.328805   \n",
       "26  0.029776  0.042443  0.052549  0.079648  0.020900  0.078980  0.033584   \n",
       "27  0.337526  0.553300  0.518870  0.559349  0.001584  0.557573  0.581518   \n",
       "28  1.000000  0.377012  0.407007  0.506015  0.001187  0.504233  0.323512   \n",
       "29  0.377012  1.000000  0.607214  0.660284  0.010434  0.657941  0.544411   \n",
       "30  0.407007  0.607214  1.000000  0.737555  0.012213  0.735187  0.523227   \n",
       "31  0.506015  0.660284  0.737555  1.000000  0.016825  0.996066  0.556650   \n",
       "32  0.001187  0.010434  0.012213  0.016825  1.000000  0.017280  0.003303   \n",
       "33  0.504233  0.657941  0.735187  0.996066  0.017280  1.000000  0.554876   \n",
       "34  0.323512  0.544411  0.523227  0.556650  0.003303  0.554876  1.000000   \n",
       "35  0.406539  0.626124  0.677790  0.729750  0.005012  0.727119  0.518704   \n",
       "36  0.014184  0.058456  0.035754  0.032132  0.047860  0.033080  0.024738   \n",
       "37  0.006926  0.003382  0.008664  0.007628  0.023289  0.007750  0.011150   \n",
       "38  0.010224  0.033171  0.040793  0.044463  0.062748  0.043915  0.021029   \n",
       "39  0.463813  0.602230  0.699918  0.848021  0.020546  0.845359  0.513130   \n",
       "40  0.015803  0.038570  0.012922  0.010166  0.002691  0.010428  0.024187   \n",
       "41  0.433659  0.015953  0.010725  0.007796  0.010443  0.008188  0.031718   \n",
       "42  0.059688  0.071130  0.087070  0.118321  0.006835  0.117510  0.063374   \n",
       "43  0.003276  0.004943  0.075289  0.004543  0.025089  0.004216  0.003104   \n",
       "44  0.010829  0.006275  0.008691  0.015739  0.001309  0.015212  0.014378   \n",
       "45  0.023578  0.011903  0.024207  0.019079  0.019852  0.019449  0.014798   \n",
       "46  0.002312  0.005045  0.002346  0.000880  0.000212  0.000707  0.016404   \n",
       "47  0.003303  0.005172  0.013711  0.011731  0.004533  0.012065  0.002160   \n",
       "48  0.018502  0.019845  0.016280  0.008853  0.005691  0.009290  0.021592   \n",
       "49  0.158593  0.224192  0.233392  0.304679  0.028655  0.303606  0.200713   \n",
       "50  0.006575  0.004667  0.010718  0.013805  0.113105  0.013687  0.034435   \n",
       "51  0.042330  0.061694  0.045273  0.041529  0.048493  0.038626  0.048822   \n",
       "52  0.050231  0.065475  0.047475  0.043484  0.048101  0.039844  0.048947   \n",
       "53  0.002076  0.082593  0.000225  0.010735  0.009928  0.010635  0.009650   \n",
       "54  0.014936  0.016599  0.010897  0.010498  0.015509  0.002404  0.013707   \n",
       "55  0.034733  0.039001  0.027449  0.027732  0.025919  0.024532  0.030236   \n",
       "56  0.056628  0.064115  0.045923  0.046796  0.006919  0.044529  0.045963   \n",
       "\n",
       "          35        36        37        38        39        40        41  \\\n",
       "0   0.054673  0.057312  0.007960  0.011134  0.036095  0.009703  0.026070   \n",
       "1   0.028198  0.024013  0.008922  0.019124  0.014821  0.015420  0.025177   \n",
       "2   0.046504  0.067015  0.032407  0.014809  0.047066  0.030956  0.005811   \n",
       "3   0.006515  0.007761  0.002669  0.004602  0.007643  0.005670  0.008095   \n",
       "4   0.048844  0.072599  0.130812  0.042044  0.021442  0.047505  0.115041   \n",
       "5   0.052819  0.057465  0.017918  0.047619  0.029866  0.029457  0.054812   \n",
       "6   0.053978  0.052035  0.014781  0.046978  0.022121  0.033120  0.049664   \n",
       "7   0.033747  0.017466  0.012119  0.030392  0.005988  0.003884  0.043626   \n",
       "8   0.056270  0.033244  0.002216  0.040844  0.009867  0.035177  0.048223   \n",
       "9   0.016955  0.004944  0.017950  0.016091  0.004163  0.025084  0.054467   \n",
       "10  0.050373  0.032310  0.004917  0.042482  0.024440  0.034654  0.043150   \n",
       "11  0.010626  0.026329  0.025483  0.018725  0.036452  0.023197  0.111335   \n",
       "12  0.046861  0.042367  0.014977  0.016401  0.013411  0.018885  0.038718   \n",
       "13  0.008924  0.030316  0.002072  0.026516  0.024623  0.020966  0.005206   \n",
       "14  0.034381  0.036186  0.011316  0.016444  0.046247  0.013241  0.031883   \n",
       "15  0.060342  0.060394  0.017720  0.035150  0.029906  0.028342  0.037275   \n",
       "16  0.031863  0.049606  0.015373  0.042276  0.009064  0.036323  0.040693   \n",
       "17  0.024925  0.039107  0.006375  0.027579  0.005743  0.029480  0.047379   \n",
       "18  0.134514  0.134785  0.032566  0.039316  0.060591  0.050328  0.087965   \n",
       "19  0.037741  0.041077  0.009671  0.028512  0.002970  0.020116  0.028773   \n",
       "20  0.105648  0.114988  0.007160  0.063244  0.016888  0.058967  0.081266   \n",
       "21  0.027201  0.033055  0.004947  0.018842  0.021537  0.014251  0.020389   \n",
       "22  0.061425  0.069769  0.016960  0.038280  0.003530  0.034898  0.049861   \n",
       "23  0.046050  0.055013  0.007913  0.033614  0.022946  0.025703  0.034146   \n",
       "24  0.386132  0.129842  0.008041  0.050343  0.325930  0.001757  0.019469   \n",
       "25  0.353362  0.159838  0.021865  0.055968  0.320140  0.016286  0.058059   \n",
       "26  0.038995  0.016729  0.009969  0.003113  0.059655  0.017477  0.005493   \n",
       "27  0.565162  0.022082  0.012962  0.042867  0.510165  0.020676  0.003988   \n",
       "28  0.406539  0.014184  0.006926  0.010224  0.463813  0.015803  0.433659   \n",
       "29  0.626124  0.058456  0.003382  0.033171  0.602230  0.038570  0.015953   \n",
       "30  0.677790  0.035754  0.008664  0.040793  0.699918  0.012922  0.010725   \n",
       "31  0.729750  0.032132  0.007628  0.044463  0.848021  0.010166  0.007796   \n",
       "32  0.005012  0.047860  0.023289  0.062748  0.020546  0.002691  0.010443   \n",
       "33  0.727119  0.033080  0.007750  0.043915  0.845359  0.010428  0.008188   \n",
       "34  0.518704  0.024738  0.011150  0.021029  0.513130  0.024187  0.031718   \n",
       "35  1.000000  0.049792  0.011167  0.070251  0.674249  0.020078  0.023001   \n",
       "36  0.049792  1.000000  0.003490  0.237294  0.017042  0.109142  0.012959   \n",
       "37  0.011167  0.003490  1.000000  0.002427  0.010190  0.006338  0.167478   \n",
       "38  0.070251  0.237294  0.002427  1.000000  0.024025  0.025061  0.036041   \n",
       "39  0.674249  0.017042  0.010190  0.024025  1.000000  0.015587  0.016903   \n",
       "40  0.020078  0.109142  0.006338  0.025061  0.015587  1.000000  0.013522   \n",
       "41  0.023001  0.012959  0.167478  0.036041  0.016903  0.013522  1.000000   \n",
       "42  0.081839  0.324884  0.006630  0.214437  0.096036  0.089370  0.017182   \n",
       "43  0.007178  0.006227  0.001771  0.014610  0.001477  0.008050  0.010357   \n",
       "44  0.010422  0.046639  0.001488  0.105905  0.008926  0.015522  0.007686   \n",
       "45  0.035040  0.127530  0.009951  0.000838  0.027591  0.349431  0.019499   \n",
       "46  0.000609  0.018507  0.003980  0.004474  0.001432  0.006999  0.013130   \n",
       "47  0.010395  0.048099  0.017660  0.009546  0.015802  0.001284  0.000529   \n",
       "48  0.018947  0.052138  0.007886  0.034492  0.018693  0.053034  0.007817   \n",
       "49  0.245454  0.107674  0.010430  0.107928  0.268701  0.017584  0.013082   \n",
       "50  0.001017  0.073010  0.001767  0.038474  0.014065  0.034408  0.011091   \n",
       "51  0.060379  0.054578  0.015126  0.024846  0.032509  0.025911  0.038094   \n",
       "52  0.057933  0.063895  0.012909  0.044513  0.016724  0.036610  0.043653   \n",
       "53  0.006452  0.022637  0.003627  0.011326  0.010661  0.011755  0.003873   \n",
       "54  0.019185  0.014423  0.006012  0.014032  0.003945  0.008895  0.017899   \n",
       "55  0.038100  0.033204  0.009487  0.029229  0.004835  0.023658  0.034585   \n",
       "56  0.045792  0.003490  0.013897  0.049256  0.028806  0.026373  0.056511   \n",
       "\n",
       "          42        43        44        45        46        47        48  \\\n",
       "0   0.024292  0.022116  0.037105  0.034056  0.000953  0.017755  0.026505   \n",
       "1   0.002370  0.019739  0.016418  0.023858  0.009818  0.015747  0.007282   \n",
       "2   0.044325  0.053464  0.050664  0.056655  0.029339  0.026344  0.033213   \n",
       "3   0.009268  0.005933  0.012957  0.009181  0.003348  0.001924  0.000591   \n",
       "4   0.048879  0.015234  0.042336  0.077986  0.026900  0.032005  0.032759   \n",
       "5   0.030616  0.028826  0.053637  0.033046  0.014343  0.031693  0.019119   \n",
       "6   0.049079  0.034461  0.050811  0.056166  0.017512  0.031408  0.033089   \n",
       "7   0.004542  0.030134  0.002423  0.037916  0.006397  0.021224  0.027432   \n",
       "8   0.034190  0.035159  0.075558  0.056817  0.007521  0.026017  0.014646   \n",
       "9   0.023200  0.026654  0.032065  0.030326  0.015546  0.016842  0.011945   \n",
       "10  0.034560  0.036533  0.064104  0.050896  0.019159  0.022689  0.032410   \n",
       "11  0.024628  0.016627  0.088843  0.071947  0.001015  0.032943  0.027711   \n",
       "12  0.012051  0.024445  0.042278  0.021946  0.012778  0.019746  0.023445   \n",
       "13  0.004633  0.012522  0.000577  0.020994  0.049647  0.016425  0.019045   \n",
       "14  0.049018  0.017571  0.038931  0.024221  0.011387  0.020818  0.018277   \n",
       "15  0.047430  0.031556  0.045653  0.047441  0.017837  0.028084  0.026841   \n",
       "16  0.052646  0.021097  0.059565  0.057915  0.007799  0.029947  0.031542   \n",
       "17  0.019975  0.032629  0.047838  0.039857  0.021317  0.017173  0.039519   \n",
       "18  0.051636  0.065022  0.098270  0.009787  0.001212  0.040015  0.044314   \n",
       "19  0.028329  0.013478  0.043708  0.030897  0.007537  0.018198  0.020851   \n",
       "20  0.050861  0.059901  0.043310  0.081662  0.012737  0.050951  0.058660   \n",
       "21  0.021567  0.011931  0.032059  0.020849  0.008436  0.013168  0.416608   \n",
       "22  0.041894  0.034337  0.053469  0.053271  0.017122  0.030155  0.027362   \n",
       "23  0.039172  0.023522  0.045922  0.031401  0.010458  0.019171  0.019139   \n",
       "24  0.114237  0.001717  0.049676  0.045916  0.002627  0.002739  0.029181   \n",
       "25  0.126805  0.015347  0.007137  0.037212  0.033909  0.032860  0.013558   \n",
       "26  0.010786  0.013011  0.006860  0.039175  0.010085  0.007194  0.022724   \n",
       "27  0.083526  0.000017  0.003867  0.008822  0.003955  0.004689  0.025020   \n",
       "28  0.059688  0.003276  0.010829  0.023578  0.002312  0.003303  0.018502   \n",
       "29  0.071130  0.004943  0.006275  0.011903  0.005045  0.005172  0.019845   \n",
       "30  0.087070  0.075289  0.008691  0.024207  0.002346  0.013711  0.016280   \n",
       "31  0.118321  0.004543  0.015739  0.019079  0.000880  0.011731  0.008853   \n",
       "32  0.006835  0.025089  0.001309  0.019852  0.000212  0.004533  0.005691   \n",
       "33  0.117510  0.004216  0.015212  0.019449  0.000707  0.012065  0.009290   \n",
       "34  0.063374  0.003104  0.014378  0.014798  0.016404  0.002160  0.021592   \n",
       "35  0.081839  0.007178  0.010422  0.035040  0.000609  0.010395  0.018947   \n",
       "36  0.324884  0.006227  0.046639  0.127530  0.018507  0.048099  0.052138   \n",
       "37  0.006630  0.001771  0.001488  0.009951  0.003980  0.017660  0.007886   \n",
       "38  0.214437  0.014610  0.105905  0.000838  0.004474  0.009546  0.034492   \n",
       "39  0.096036  0.001477  0.008926  0.027591  0.001432  0.015802  0.018693   \n",
       "40  0.089370  0.008050  0.015522  0.349431  0.006999  0.001284  0.053034   \n",
       "41  0.017182  0.010357  0.007686  0.019499  0.013130  0.000529  0.007817   \n",
       "42  1.000000  0.008684  0.088772  0.014189  0.017961  0.006238  0.015385   \n",
       "43  0.008684  1.000000  0.004127  0.015001  0.003601  0.000808  0.007257   \n",
       "44  0.088772  0.004127  1.000000  0.043454  0.012407  0.005125  0.024698   \n",
       "45  0.014189  0.015001  0.043454  1.000000  0.010370  0.015719  0.015382   \n",
       "46  0.017961  0.003601  0.012407  0.010370  1.000000  0.006349  0.000995   \n",
       "47  0.006238  0.000808  0.005125  0.015719  0.006349  1.000000  0.002290   \n",
       "48  0.015385  0.007257  0.024698  0.015382  0.000995  0.002290  1.000000   \n",
       "49  0.060568  0.003203  0.001413  0.014763  0.003085  0.012795  0.049124   \n",
       "50  0.115548  0.010733  0.008838  0.003168  0.004592  0.006310  0.009070   \n",
       "51  0.049362  0.033837  0.067569  0.028845  0.017679  0.026576  0.020539   \n",
       "52  0.054698  0.036241  0.049367  0.050109  0.018549  0.030751  0.006392   \n",
       "53  0.013925  0.001167  0.023878  0.015040  0.000308  0.008575  0.055057   \n",
       "54  0.017681  0.013157  0.026979  0.017408  0.006465  0.008114  0.003443   \n",
       "55  0.017279  0.025918  0.051858  0.033365  0.010154  0.016894  0.040829   \n",
       "56  0.036529  0.040661  0.095444  0.046371  0.005158  0.010033  0.055298   \n",
       "\n",
       "          49        50        51        52        53        54        55  \\\n",
       "0   0.021196  0.033301  0.058292  0.117419  0.008844  0.044491  0.061382   \n",
       "1   0.049837  0.018527  0.014461  0.009605  0.001946  0.002083  0.000271   \n",
       "2   0.016495  0.033120  0.108140  0.087618  0.003336  0.097398  0.107463   \n",
       "3   0.012370  0.007148  0.003138  0.010862  0.000298  0.005260  0.022081   \n",
       "4   0.046361  0.026390  0.025509  0.041582  0.002016  0.052662  0.052290   \n",
       "5   0.008705  0.015133  0.065043  0.105692  0.019894  0.010278  0.090172   \n",
       "6   0.051885  0.027653  0.053706  0.070127  0.046612  0.041565  0.059677   \n",
       "7   0.032494  0.019548  0.031454  0.057910  0.008012  0.011254  0.037575   \n",
       "8   0.031003  0.013601  0.043639  0.149365  0.000522  0.111308  0.189247   \n",
       "9   0.003936  0.007357  0.036737  0.075786  0.044830  0.073677  0.103308   \n",
       "10  0.055089  0.025183  0.024992  0.070227  0.001126  0.029258  0.086791   \n",
       "11  0.030940  0.044966  0.013369  0.016723  0.030445  0.010002  0.021774   \n",
       "12  0.051151  0.028283  0.040737  0.205905  0.014195  0.013446  0.041962   \n",
       "13  0.005804  0.014349  0.008499  0.080953  0.006545  0.003023  0.060993   \n",
       "14  0.002551  0.003111  0.018607  0.123854  0.005446  0.017383  0.213992   \n",
       "15  0.046578  0.029560  0.104261  0.049953  0.035534  0.015036  0.026528   \n",
       "16  0.035897  0.036691  0.077049  0.098323  0.000466  0.038126  0.062672   \n",
       "17  0.035897  0.017439  0.039350  0.063872  0.020978  0.007979  0.075122   \n",
       "18  0.128882  0.063826  0.153381  0.091470  0.002434  0.030592  0.006530   \n",
       "19  0.021431  0.012071  0.048350  0.034948  0.007214  0.067140  0.099463   \n",
       "20  0.085181  0.045469  0.084017  0.141649  0.004355  0.041066  0.085321   \n",
       "21  0.046244  0.001137  0.004838  0.011036  0.184428  0.021497  0.027775   \n",
       "22  0.033174  0.000467  0.070103  0.310971  0.020140  0.008372  0.123036   \n",
       "23  0.033113  0.020798  0.051076  0.104691  0.000703  0.007681  0.044870   \n",
       "24  0.136979  0.039723  0.090862  0.086634  0.058780  0.017285  0.051206   \n",
       "25  0.144771  0.064349  0.078367  0.081198  0.020691  0.024234  0.051806   \n",
       "26  0.028748  0.017676  0.067500  0.068728  0.020561  0.025504  0.054400   \n",
       "27  0.313835  0.031979  0.063495  0.061441  0.011438  0.013757  0.038772   \n",
       "28  0.158593  0.006575  0.042330  0.050231  0.002076  0.014936  0.034733   \n",
       "29  0.224192  0.004667  0.061694  0.065475  0.082593  0.016599  0.039001   \n",
       "30  0.233392  0.010718  0.045273  0.047475  0.000225  0.010897  0.027449   \n",
       "31  0.304679  0.013805  0.041529  0.043484  0.010735  0.010498  0.027732   \n",
       "32  0.028655  0.113105  0.048493  0.048101  0.009928  0.015509  0.025919   \n",
       "33  0.303606  0.013687  0.038626  0.039844  0.010635  0.002404  0.024532   \n",
       "34  0.200713  0.034435  0.048822  0.048947  0.009650  0.013707  0.030236   \n",
       "35  0.245454  0.001017  0.060379  0.057933  0.006452  0.019185  0.038100   \n",
       "36  0.107674  0.073010  0.054578  0.063895  0.022637  0.014423  0.033204   \n",
       "37  0.010430  0.001767  0.015126  0.012909  0.003627  0.006012  0.009487   \n",
       "38  0.107928  0.038474  0.024846  0.044513  0.011326  0.014032  0.029229   \n",
       "39  0.268701  0.014065  0.032509  0.016724  0.010661  0.003945  0.004835   \n",
       "40  0.017584  0.034408  0.025911  0.036610  0.011755  0.008895  0.023658   \n",
       "41  0.013082  0.011091  0.038094  0.043653  0.003873  0.017899  0.034585   \n",
       "42  0.060568  0.115548  0.049362  0.054698  0.013925  0.017681  0.017279   \n",
       "43  0.003203  0.010733  0.033837  0.036241  0.001167  0.013157  0.025918   \n",
       "44  0.001413  0.008838  0.067569  0.049367  0.023878  0.026979  0.051858   \n",
       "45  0.014763  0.003168  0.028845  0.050109  0.015040  0.017408  0.033365   \n",
       "46  0.003085  0.004592  0.017679  0.018549  0.000308  0.006465  0.010154   \n",
       "47  0.012795  0.006310  0.026576  0.030751  0.008575  0.008114  0.016894   \n",
       "48  0.049124  0.009070  0.020539  0.006392  0.055057  0.003443  0.040829   \n",
       "49  1.000000  0.022316  0.030354  0.044722  0.023322  0.034365  0.370963   \n",
       "50  0.022316  1.000000  0.031769  0.026400  0.006863  0.008180  0.013994   \n",
       "51  0.030354  0.031769  1.000000  0.142913  0.020924  0.054308  0.077392   \n",
       "52  0.044722  0.026400  0.142913  1.000000  0.012613  0.079998  0.183144   \n",
       "53  0.023322  0.006863  0.020924  0.012613  1.000000  0.013497  0.061657   \n",
       "54  0.034365  0.008180  0.054308  0.079998  0.013497  1.000000  0.492638   \n",
       "55  0.370963  0.013994  0.077392  0.183144  0.061657  0.492638  1.000000   \n",
       "56  0.112209  0.006016  0.036321  0.201948  0.042568  0.162314  0.475486   \n",
       "\n",
       "          56  \n",
       "0   0.089165  \n",
       "1   0.022680  \n",
       "2   0.070114  \n",
       "3   0.021369  \n",
       "4   0.002492  \n",
       "5   0.082089  \n",
       "6   0.008344  \n",
       "7   0.040252  \n",
       "8   0.248724  \n",
       "9   0.087273  \n",
       "10  0.115055  \n",
       "11  0.020076  \n",
       "12  0.105150  \n",
       "13  0.169257  \n",
       "14  0.151626  \n",
       "15  0.003007  \n",
       "16  0.064261  \n",
       "17  0.046364  \n",
       "18  0.007307  \n",
       "19  0.075751  \n",
       "20  0.051797  \n",
       "21  0.103954  \n",
       "22  0.165977  \n",
       "23  0.080993  \n",
       "24  0.043267  \n",
       "25  0.059601  \n",
       "26  0.096548  \n",
       "27  0.067596  \n",
       "28  0.056628  \n",
       "29  0.064115  \n",
       "30  0.045923  \n",
       "31  0.046796  \n",
       "32  0.006919  \n",
       "33  0.044529  \n",
       "34  0.045963  \n",
       "35  0.045792  \n",
       "36  0.003490  \n",
       "37  0.013897  \n",
       "38  0.049256  \n",
       "39  0.028806  \n",
       "40  0.026373  \n",
       "41  0.056511  \n",
       "42  0.036529  \n",
       "43  0.040661  \n",
       "44  0.095444  \n",
       "45  0.046371  \n",
       "46  0.005158  \n",
       "47  0.010033  \n",
       "48  0.055298  \n",
       "49  0.112209  \n",
       "50  0.006016  \n",
       "51  0.036321  \n",
       "52  0.201948  \n",
       "53  0.042568  \n",
       "54  0.162314  \n",
       "55  0.475486  \n",
       "56  1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = df1.drop(57,1).corr().abs()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(57,1).corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 57)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(np.ones((3,3)), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 high corr F2\n",
    "F3 high corr F2\n",
    "F1 weak corr F1\n",
    "\n",
    "VIF\n",
    "corr(F1, F2, F3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1, VIF = 5\n",
    "F2, VIF = 4.8\n",
    "\n",
    "Target, F1\n",
    "Target, F2 = >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be dropped: \n",
      "[33, 39]\n"
     ]
    }
   ],
   "source": [
    "# Remove Correlated features above 0.75 and then apply logistic model\n",
    "corr_matrix = df1.drop(57,1).corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "\n",
    "print(\"Columns to be dropped: \")\n",
    "print(to_drop)\n",
    "df1.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 56)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2    3     4     5     6     7     8     9    10    11  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  0.00  0.64   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  0.21  0.79   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  0.38  0.45   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31   \n",
       "\n",
       "     12    13    14    15    16    17    18    19    20   21    22    23   24  \\\n",
       "0  0.00  0.00  0.00  0.32  0.00  1.29  1.93  0.00  0.96  0.0  0.00  0.00  0.0   \n",
       "1  0.65  0.21  0.14  0.14  0.07  0.28  3.47  0.00  1.59  0.0  0.43  0.43  0.0   \n",
       "2  0.12  0.00  1.75  0.06  0.06  1.03  1.36  0.32  0.51  0.0  1.16  0.06  0.0   \n",
       "3  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0   \n",
       "4  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0   \n",
       "\n",
       "    25   26   27   28   29   30   31   32   34   35    36   37   38   40   41  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.07  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.0  0.0   \n",
       "\n",
       "     42   43    44    45   46   47    48     49   50     51     52     53  \\\n",
       "0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.000  0.0  0.778  0.000  0.000   \n",
       "1  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.132  0.0  0.372  0.180  0.048   \n",
       "2  0.12  0.0  0.06  0.06  0.0  0.0  0.01  0.143  0.0  0.276  0.184  0.010   \n",
       "3  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.137  0.0  0.137  0.000  0.000   \n",
       "4  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.135  0.0  0.135  0.000  0.000   \n",
       "\n",
       "      54   55    56  57  \n",
       "0  3.756  61   278   1   \n",
       "1  5.114  101  1028  1   \n",
       "2  9.821  485  2259  1   \n",
       "3  3.537  40   191   1   \n",
       "4  3.537  40   191   1   "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split the  new subset of the  data acquired by feature selection into train and test set and fit the logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the new subset of data and fit the logistic model on training data\n",
    "X = df1.iloc[:,:-1]\n",
    "y = df1.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 0)\n",
    "\n",
    "sc = StandardScaler() \n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test) \n",
    "\n",
    "lr = LogisticRegression(random_state=101)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 55)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9094858797972484\n",
      "Confusion Matrix: \n",
      " [[773  49]\n",
      " [ 76 483]]\n",
      "========================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93       822\n",
      "           1       0.91      0.86      0.89       559\n",
      "\n",
      "    accuracy                           0.91      1381\n",
      "   macro avg       0.91      0.90      0.91      1381\n",
      "weighted avg       0.91      0.91      0.91      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix for new data\n",
    "print(\"Accuracy on test data:\", lr.score(X_test,y_test))\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"==\"*20)\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold > 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.605955\n",
       "1    0.394045\n",
       "Name: 57, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. After keeping highly correlated features, there is not much change in the score. Lets apply another feature selection technique(Chi Squared test) to see whether we can increase our score. Find the optimum number of features using Chi Square and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature and your target are categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.9015206372194062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 25 , score= 0.9116582186821144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 30 , score= 0.9102099927588704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 35 , score= 0.9225199131064447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 40 , score= 0.9225199131064447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 50 , score= 0.9232440260680667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 55 , score= 0.9210716871832005\n",
      " \n",
      "High Score is: 0.9232440260680667 with features= 50\n"
     ]
    }
   ],
   "source": [
    "# Apply Chi Square and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    test = SelectKBest(score_func=chi2, k= n )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "    \n",
    "#     sc = StandardScaler() \n",
    "#     X_train = sc.fit_transform(X_train) \n",
    "#     X_test = sc.transform(X_test) \n",
    "    \n",
    "    X_train = test.fit_transform(X_train,y_train)\n",
    "    X_test = test.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(random_state=101)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    print(\"For no of features=\",n,\", score=\", model.score(X_test,y_test))\n",
    "    if model.score(X_test,y_test)>high_score:\n",
    "        high_score=model.score(X_test,y_test)\n",
    "        nof=n \n",
    "\n",
    "print(\" \")\n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Find out the accuracy , print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[746  58]\n",
      " [ 51 526]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Using chi squared test there is no change in the score and the optimum features that we got is 55. Now lets see if we can increase our score using another feature selection technique called Anova.Find the optimum number of features using Anova and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 55)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.887762490948588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 25 , score= 0.9000724112961622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 30 , score= 0.9131064446053584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 35 , score= 0.9167270094134685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 40 , score= 0.9181752353367125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 50 , score= 0.9196234612599565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 55 , score= 0.9217958001448225\n",
      " \n",
      "High Score is: 0.9217958001448225 with features= 55\n"
     ]
    }
   ],
   "source": [
    "# Apply Anova and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    test = SelectKBest(score_func=f_classif , k= n )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "    \n",
    "    sc = StandardScaler() \n",
    "    X_train = sc.fit_transform(X_train) \n",
    "    X_test = sc.transform(X_test) \n",
    "\n",
    "    X_train = test.fit_transform(X_train, y_train)\n",
    "    X_test = test.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", model.score(X_test,y_test))\n",
    "\n",
    "    if model.score(X_test,y_test)>high_score:\n",
    "        high_score=model.score(X_test,y_test)\n",
    "        model_ = model\n",
    "        nof=n \n",
    "        \n",
    "print(\" \")\n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Find out the accuracy , print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 55 , score= 0.9217958001448225\n"
     ]
    }
   ],
   "source": [
    "test = SelectKBest(score_func=f_classif , k= 55 )\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 2020)\n",
    "\n",
    "sc = StandardScaler() \n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test) \n",
    "\n",
    "X_train = test.fit_transform(X_train, y_train)\n",
    "X_test = test.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "print(\"For no of features=\",n,\", score=\", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[767  37]\n",
      " [ 71 506]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       804\n",
      "           1       0.93      0.88      0.90       577\n",
      "\n",
      "    accuracy                           0.92      1381\n",
      "   macro avg       0.92      0.92      0.92      1381\n",
      "weighted avg       0.92      0.92      0.92      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy on test data: 0.9094858797972484\n",
    "Confusion Matrix: \n",
    " [[773  49]\n",
    " [ 76 483]]\n",
    "========================================\n",
    "Classification Report: \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.94      0.93       822\n",
    "           1       0.91      0.86      0.89       559\n",
    "\n",
    "    accuracy                           0.91      1381\n",
    "   macro avg       0.91      0.90      0.91      1381\n",
    "weighted avg       0.91      0.91      0.91      1381\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Unfortunately Anova also couldn't give us a better score . Let's finally attempt PCA on train data and find if it helps in  giving a better model by reducing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation explained: [0.09501561 0.15290854 0.19082799 0.22072453 0.24854445 0.27502635\n",
      " 0.30053834 0.32515689 0.34855086 0.37114944 0.39259994 0.41319682\n",
      " 0.43298816 0.45235705 0.47144728 0.49027067 0.50857914 0.5266847\n",
      " 0.54476794 0.5626464 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.9022447501810282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=25, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation explained: [0.09501561 0.15290852 0.19082986 0.22073527 0.24856572 0.27503211\n",
      " 0.30061218 0.32529496 0.34873994 0.37132123 0.39280499 0.41359723\n",
      " 0.43360136 0.45317708 0.47233521 0.49116671 0.50964564 0.5278241\n",
      " 0.54588451 0.56390998 0.58165059 0.59920852 0.61658262 0.63355924\n",
      " 0.65014837]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 25 , score= 0.9065894279507604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=30, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation explained: [0.09501561 0.15290863 0.19083152 0.22074501 0.24857418 0.27506417\n",
      " 0.30067866 0.32537999 0.34886947 0.37153114 0.39309094 0.41394581\n",
      " 0.43403095 0.45367034 0.47307555 0.4922571  0.51089465 0.52932581\n",
      " 0.54753634 0.56569206 0.58372154 0.6015335  0.61915189 0.63664451\n",
      " 0.65407128 0.67113849 0.68778728 0.70406147 0.71977898 0.7349825 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 30 , score= 0.9065894279507604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=35, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation explained: [0.09501561 0.15290864 0.19083152 0.22074486 0.24858314 0.27507674\n",
      " 0.30069316 0.32540156 0.34890273 0.37157688 0.39315248 0.41402876\n",
      " 0.43414653 0.45379445 0.47321364 0.49243432 0.51113944 0.5296269\n",
      " 0.54789074 0.56609884 0.58415458 0.60205727 0.61975578 0.6372736\n",
      " 0.65471716 0.67185215 0.68859598 0.70493601 0.72081347 0.73647789\n",
      " 0.75186411 0.76698782 0.78149482 0.79536927 0.80917693]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 35 , score= 0.9094858797972484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=40, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation explained: [0.09501561 0.15290864 0.19083168 0.22074559 0.24858441 0.27507811\n",
      " 0.30069708 0.3254062  0.34890984 0.37158648 0.39316773 0.41405444\n",
      " 0.43417594 0.45383942 0.47327241 0.49250509 0.51121592 0.52971637\n",
      " 0.54799203 0.56622342 0.58428049 0.60218646 0.61988892 0.63741701\n",
      " 0.65486491 0.67202246 0.68877678 0.70512767 0.72104869 0.73678033\n",
      " 0.75216565 0.76738327 0.78191005 0.7959209  0.80980416 0.82348238\n",
      " 0.83689872 0.84980482 0.86231337 0.87455902]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 40 , score= 0.9138305575669804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=50, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation explained: [0.09501561 0.15290864 0.19083171 0.22074583 0.24858473 0.27507889\n",
      " 0.30069788 0.32540727 0.34891157 0.3715883  0.39317013 0.41405749\n",
      " 0.4341798  0.45384364 0.4732779  0.49251088 0.51122314 0.52972913\n",
      " 0.54800574 0.56623891 0.58429921 0.60220786 0.61991158 0.63744237\n",
      " 0.65490111 0.67205892 0.68882459 0.70517805 0.72110309 0.73683502\n",
      " 0.75222807 0.76745148 0.78198214 0.79599572 0.80988604 0.82356965\n",
      " 0.8369898  0.84991758 0.8624425  0.87472258 0.88678537 0.89843069\n",
      " 0.90949044 0.92027773 0.93051813 0.94041446 0.94974258 0.95821851\n",
      " 0.96616137 0.97375063]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 50 , score= 0.9167270094134685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=55, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation explained: [0.09501561 0.15290864 0.19083171 0.22074583 0.24858473 0.27507889\n",
      " 0.30069788 0.32540727 0.34891157 0.3715883  0.39317013 0.41405749\n",
      " 0.4341798  0.45384364 0.4732779  0.49251088 0.51122314 0.52972913\n",
      " 0.54800574 0.56623891 0.58429921 0.60220786 0.61991158 0.63744237\n",
      " 0.65490111 0.67205892 0.68882459 0.70517805 0.72110309 0.73683502\n",
      " 0.75222807 0.76745148 0.78198214 0.79599572 0.80988604 0.82356965\n",
      " 0.8369898  0.84991758 0.8624425  0.87472258 0.88678537 0.89843069\n",
      " 0.90949044 0.92027773 0.93051813 0.94041446 0.94974258 0.95821851\n",
      " 0.96616137 0.97375063 0.9812967  0.98790349 0.9928925  0.99722561\n",
      " 1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 55 , score= 0.9217958001448225\n",
      "High Score is: 0.9217958001448225 with features= 55\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "    \n",
    "    sc = StandardScaler() \n",
    "    X_train = sc.fit_transform(X_train) \n",
    "    X_test = sc.transform(X_test) \n",
    "\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train)\n",
    "    print(\"Variation explained: {}\".format(np.cumsum(pca.explained_variance_ratio_)))\n",
    "    \n",
    "    \n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    logistic = LogisticRegression()\n",
    "    logistic.fit(X_train, y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", logistic.score(X_test,y_test))\n",
    "    \n",
    "    if logistic.score(X_test,y_test)>high_score:\n",
    "        high_score=logistic.score(X_test,y_test)\n",
    "        logistic_ = logistic\n",
    "        nof=n \n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=57, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "sc = StandardScaler() \n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test) \n",
    "\n",
    "pca = PCA(n_components=57)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11590037, 0.17436497, 0.21125924, 0.24012915, 0.26710226,\n",
       "       0.29267004, 0.31749918, 0.34141509, 0.36409998, 0.38601875,\n",
       "       0.40745605, 0.42774099, 0.44749239, 0.46647699, 0.48533009,\n",
       "       0.50391815, 0.52200111, 0.53985872, 0.55749651, 0.57509585,\n",
       "       0.59253341, 0.60981927, 0.62690216, 0.64387778, 0.66077666,\n",
       "       0.67733812, 0.69351574, 0.70941375, 0.72487335, 0.74005544,\n",
       "       0.75505275, 0.76975063, 0.78378736, 0.79749661, 0.81099697,\n",
       "       0.82428495, 0.83724966, 0.84990474, 0.86220617, 0.87413729,\n",
       "       0.88578254, 0.89709267, 0.90776781, 0.91821286, 0.92810021,\n",
       "       0.93765645, 0.94666517, 0.95497495, 0.96272465, 0.97012563,\n",
       "       0.97743874, 0.98382046, 0.98982537, 0.99404003, 0.99759018,\n",
       "       0.99989573, 1.        ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(42,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pca.explained_variance_ratio_\n",
    "#np.sum(pca.explained_variance_ratio_)\n",
    "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "cum_var\n",
    "cum_var[cum_var < 0.90].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 55 , score= 0.9225199131064447\n"
     ]
    }
   ],
   "source": [
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "print(\"For no of features=\",n,\", score=\", logistic.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Find out the accuracy , print out the Confusion Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[768  36]\n",
      " [ 71 506]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.93       804\n",
      "           1       0.93      0.88      0.90       577\n",
      "\n",
      "    accuracy                           0.92      1381\n",
      "   macro avg       0.92      0.92      0.92      1381\n",
      "weighted avg       0.92      0.92      0.92      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred = logistic.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. You can also compare your predicted values and observed values by printing out values of logistic.predict(X_test[]) and  y_test[].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 10 observation:     [0 0 0 1 0 1 0 0 0 1]\n",
      "Actual values for 10 observation:  [0 0 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Compare observed value and Predicted value\n",
    "print(\"Prediction for 10 observation:    \",logistic.predict(X_test[0:10]))\n",
    "print(\"Actual values for 10 observation: \",y_test[0:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "# In-session Recap Time\n",
    "***\n",
    "* Feature Selection\n",
    "* Univariate Feature Selection\n",
    "* Multivariate Feature Selection\n",
    "* Filter Methods\n",
    "* Wrapper Methods\n",
    "* Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You\n",
    "***\n",
    "### Next Session: Decision Trees"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
